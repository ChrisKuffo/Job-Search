{
  "crawl4ai_documentation": [
    {
      "title": "Introduction",
      "url": "https://crawl4ai.com/mkdocs/introduction/",
      "content": "# Introduction\n\nWelcome to the documentation for Crawl4AI v0.2.5! ðŸ•·ï¸ðŸ¤–\n\nCrawl4AI is designed to simplify the process of crawling web pages and\nextracting useful information for large language models (LLMs) and AI\napplications. Whether you're using it as a REST API, a Python library, or\nthrough a Google Colab notebook, Crawl4AI provides powerful features to make\nweb data extraction easier and more efficient.\n\n## Key Features âœ¨\n\n  * **ðŸ†“ Completely Free and Open-Source** : Crawl4AI is free to use and open-source, making it accessible for everyone.\n  * **ðŸ¤– LLM-Friendly Output Formats** : Supports JSON, cleaned HTML, and markdown formats.\n  * **ðŸŒ Concurrent Crawling** : Crawl multiple URLs simultaneously to save time.\n  * **ðŸŽ¨ Media Extraction** : Extract all media tags including images, audio, and video.\n  * **ðŸ”— Link Extraction** : Extract all external and internal links from web pages.\n  * **ðŸ“š Metadata Extraction** : Extract metadata from web pages for additional context.\n  * **ðŸ”„ Custom Hooks** : Define custom hooks for authentication, headers, and page modifications before crawling.\n  * **ðŸ•µï¸ User Agent Support** : Customize the user agent for HTTP requests.\n  * **ðŸ–¼ï¸ Screenshot Capability** : Take screenshots of web pages during crawling.\n  * **ðŸ“œ JavaScript Execution** : Execute custom JavaScripts before crawling.\n  * **ðŸ“š Advanced Chunking and Extraction Strategies** : Utilize topic-based, regex, sentence chunking, cosine clustering, and LLM extraction strategies.\n  * **ðŸŽ¯ CSS Selector Support** : Extract specific content using CSS selectors.\n  * **ðŸ“ Instruction/Keyword Refinement** : Pass instructions or keywords to refine the extraction process.\n\nCheck the\n[Changelog](https://github.com/unclecode/crawl4ai/blob/main/CHANGELOG.md) for\nmore details.\n\n## Power and Simplicity of Crawl4AI ðŸš€\n\nCrawl4AI provides an easy way to crawl and extract data from web pages without\ninstalling any library. You can use the REST API on our server or run the\nlocal server on your machine. For more advanced control, use the Python\nlibrary to customize your crawling and extraction strategies.\n\nExplore the documentation to learn more about the features, installation\nprocess, usage examples, and how to contribute to Crawl4AI. Let's make the web\nmore accessible and useful for AI applications! ðŸ’ªðŸŒðŸ¤–\n\n"
    },
    {
      "title": "Installation",
      "url": "https://crawl4ai.com/mkdocs/installation/",
      "content": "# Installation ðŸ’»\n\nCrawl4AI offers flexible installation options to suit various use cases. You\ncan install it as a Python package, use it with Docker, or run it as a local\nserver.\n\n## Option 1: Python Package Installation (Recommended)\n\nCrawl4AI is now available on PyPI, making installation easier than ever.\nChoose the option that best fits your needs:\n\n### Basic Installation\n\nFor basic web crawling and scraping tasks:\n\n    \n    \n    pip install crawl4ai\n    playwright install # Install Playwright dependencies\n    \n\n### Installation with PyTorch\n\nFor advanced text clustering (includes CosineSimilarity cluster strategy):\n\n    \n    \n    pip install crawl4ai[torch]\n    \n\n### Installation with Transformers\n\nFor text summarization and Hugging Face models:\n\n    \n    \n    pip install crawl4ai[transformer]\n    \n\n### Full Installation\n\nFor all features:\n\n    \n    \n    pip install crawl4ai[all]\n    \n\n### Development Installation\n\nFor contributors who plan to modify the source code:\n\n    \n    \n    git clone https://github.com/unclecode/crawl4ai.git\n    cd crawl4ai\n    pip install -e \".[all]\"\n    playwright install # Install Playwright dependencies\n    \n\nðŸ’¡ After installation with \"torch\", \"transformer\", or \"all\" options, it's\nrecommended to run the following CLI command to load the required models:\n\n    \n    \n    crawl4ai-download-models\n    \n\nThis is optional but will boost the performance and speed of the crawler. You\nonly need to do this once after installation.\n\n## Option 2: Using Docker (Coming Soon)\n\nDocker support for Crawl4AI is currently in progress and will be available\nsoon. This will allow you to run Crawl4AI in a containerized environment,\nensuring consistency across different systems.\n\n## Option 3: Local Server Installation\n\nFor those who prefer to run Crawl4AI as a local server, instructions will be\nprovided once the Docker implementation is complete.\n\n## Verifying Your Installation\n\nAfter installation, you can verify that Crawl4AI is working correctly by\nrunning a simple Python script:\n\n    \n    \n    import asyncio\n    from crawl4ai import AsyncWebCrawler\n    \n    async def main():\n        async with AsyncWebCrawler(verbose=True) as crawler:\n            result = await crawler.arun(url=\"https://www.example.com\")\n            print(result.markdown[:500])  # Print first 500 characters\n    \n    if __name__ == \"__main__\":\n        asyncio.run(main())\n    \n\nThis script should successfully crawl the example website and print the first\n500 characters of the extracted content.\n\n## Getting Help\n\nIf you encounter any issues during installation or usage, please check the\n[documentation](https://crawl4ai.com/mkdocs/) or raise an issue on the [GitHub\nrepository](https://github.com/unclecode/crawl4ai/issues).\n\nHappy crawling! ðŸ•·ï¸ðŸ¤–\n\n"
    },
    {
      "title": "Quick Start",
      "url": "https://crawl4ai.com/mkdocs/quickstart/",
      "content": "# Quick Start Guide ðŸš€\n\nWelcome to the Crawl4AI Quickstart Guide! In this tutorial, we'll walk you\nthrough the basic usage of Crawl4AI with a friendly and humorous tone. We'll\ncover everything from basic usage to advanced features like chunking and\nextraction strategies, all with the power of asynchronous programming. Let's\ndive in! ðŸŒŸ\n\n## Getting Started ðŸ› ï¸\n\nFirst, let's import the necessary modules and create an instance of\n`AsyncWebCrawler`. We'll use an async context manager, which handles the setup\nand teardown of the crawler for us.\n\n    \n    \n    import asyncio\n    from crawl4ai import AsyncWebCrawler\n    \n    async def main():\n        async with AsyncWebCrawler(verbose=True) as crawler:\n            # We'll add our crawling code here\n            pass\n    \n    if __name__ == \"__main__\":\n        asyncio.run(main())\n    \n\n### Basic Usage\n\nSimply provide a URL and let Crawl4AI do the magic!\n\n    \n    \n    async def main():\n        async with AsyncWebCrawler(verbose=True) as crawler:\n            result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n            print(f\"Basic crawl result: {result.markdown[:500]}\")  # Print first 500 characters\n    \n    asyncio.run(main())\n    \n\n### Taking Screenshots ðŸ“¸\n\nLet's take a screenshot of the page!\n\n    \n    \n    import base64\n    \n    async def main():\n        async with AsyncWebCrawler(verbose=True) as crawler:\n            result = await crawler.arun(url=\"https://www.nbcnews.com/business\", screenshot=True)\n            with open(\"screenshot.png\", \"wb\") as f:\n                f.write(base64.b64decode(result.screenshot))\n            print(\"Screenshot saved to 'screenshot.png'!\")\n    \n    asyncio.run(main())\n    \n\n### Understanding Parameters ðŸ§ \n\nBy default, Crawl4AI caches the results of your crawls. This means that\nsubsequent crawls of the same URL will be much faster! Let's see this in\naction.\n\n    \n    \n    async def main():\n        async with AsyncWebCrawler(verbose=True) as crawler:\n            # First crawl (caches the result)\n            result1 = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n            print(f\"First crawl result: {result1.markdown[:100]}...\")\n    \n            # Force to crawl again\n            result2 = await crawler.arun(url=\"https://www.nbcnews.com/business\", bypass_cache=True)\n            print(f\"Second crawl result: {result2.markdown[:100]}...\")\n    \n    asyncio.run(main())\n    \n\n### Adding a Chunking Strategy ðŸ§©\n\nLet's add a chunking strategy: `RegexChunking`! This strategy splits the text\nbased on a given regex pattern.\n\n    \n    \n    from crawl4ai.chunking_strategy import RegexChunking\n    \n    async def main():\n        async with AsyncWebCrawler(verbose=True) as crawler:\n            result = await crawler.arun(\n                url=\"https://www.nbcnews.com/business\",\n                chunking_strategy=RegexChunking(patterns=[\"\\n\\n\"])\n            )\n            print(f\"RegexChunking result: {result.extracted_content[:200]}...\")\n    \n    asyncio.run(main())\n    \n\n### Adding an Extraction Strategy ðŸ§ \n\nLet's get smarter with an extraction strategy: `JsonCssExtractionStrategy`!\nThis strategy extracts structured data from HTML using CSS selectors.\n\n    \n    \n    from crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n    import json\n    \n    async def main():\n        schema = {\n            \"name\": \"News Articles\",\n            \"baseSelector\": \"article.tease-card\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"h2\",\n                    \"type\": \"text\",\n                },\n                {\n                    \"name\": \"summary\",\n                    \"selector\": \"div.tease-card__info\",\n                    \"type\": \"text\",\n                }\n            ],\n        }\n    \n        async with AsyncWebCrawler(verbose=True) as crawler:\n            result = await crawler.arun(\n                url=\"https://www.nbcnews.com/business\",\n                extraction_strategy=JsonCssExtractionStrategy(schema, verbose=True)\n            )\n            extracted_data = json.loads(result.extracted_content)\n            print(f\"Extracted {len(extracted_data)} articles\")\n            print(json.dumps(extracted_data[0], indent=2))\n    \n    asyncio.run(main())\n    \n\n### Using LLMExtractionStrategy ðŸ¤–\n\nTime to bring in the big guns: `LLMExtractionStrategy`! This strategy uses a\nlarge language model to extract relevant information from the web page.\n\n    \n    \n    from crawl4ai.extraction_strategy import LLMExtractionStrategy\n    import os\n    from pydantic import BaseModel, Field\n    \n    class OpenAIModelFee(BaseModel):\n        model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n        input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n        output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n    \n    async def main():\n        if not os.getenv(\"OPENAI_API_KEY\"):\n            print(\"OpenAI API key not found. Skipping this example.\")\n            return\n    \n        async with AsyncWebCrawler(verbose=True) as crawler:\n            result = await crawler.arun(\n                url=\"https://openai.com/api/pricing/\",\n                word_count_threshold=1,\n                extraction_strategy=LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv(\"OPENAI_API_KEY\"),\n                    schema=OpenAIModelFee.schema(),\n                    extraction_type=\"schema\",\n                    instruction=\"\"\"From the crawled content, extract all mentioned model names along with their fees for input and output tokens. \n                    Do not miss any models in the entire content. One extracted model JSON format should look like this: \n                    {\"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\"}.\"\"\",\n                ),\n                bypass_cache=True,\n            )\n            print(result.extracted_content)\n    \n    asyncio.run(main())\n    \n\n### Interactive Extraction ðŸ–±ï¸\n\nLet's use JavaScript to interact with the page before extraction!\n\n    \n    \n    async def main():\n        js_code = \"\"\"\n        const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n        loadMoreButton && loadMoreButton.click();\n        \"\"\"\n    \n        wait_for = \"\"\"() => {\n            return Array.from(document.querySelectorAll('article.tease-card')).length > 10;\n        }\"\"\"\n    \n        async with AsyncWebCrawler(verbose=True) as crawler:\n            result = await crawler.arun(\n                url=\"https://www.nbcnews.com/business\",\n                js_code=js_code,\n                wait_for=wait_for,\n                css_selector=\"article.tease-card\",\n                bypass_cache=True,\n            )\n            print(f\"JavaScript interaction result: {result.extracted_content[:500]}\")\n    \n    asyncio.run(main())\n    \n\n### Advanced Session-Based Crawling with Dynamic Content ðŸ”„\n\nIn modern web applications, content is often loaded dynamically without\nchanging the URL. This is common in single-page applications (SPAs) or\nwebsites using infinite scrolling. Traditional crawling methods that rely on\nURL changes won't work here. That's where Crawl4AI's advanced session-based\ncrawling comes in handy!\n\nHere's what makes this approach powerful:\n\n  1. **Session Preservation** : By using a `session_id`, we can maintain the state of our crawling session across multiple interactions with the page. This is crucial for navigating through dynamically loaded content.\n\n  2. **Asynchronous JavaScript Execution** : We can execute custom JavaScript to trigger content loading or navigation. In this example, we'll click a \"Load More\" button to fetch the next page of commits.\n\n  3. **Dynamic Content Waiting** : The `wait_for` parameter allows us to specify a condition that must be met before considering the page load complete. This ensures we don't extract data before the new content is fully loaded.\n\nLet's see how this works with a real-world example: crawling multiple pages of\ncommits on a GitHub repository. The URL doesn't change as we load more\ncommits, so we'll use these advanced techniques to navigate and extract data.\n\n    \n    \n    import json\n    from bs4 import BeautifulSoup\n    from crawl4ai import AsyncWebCrawler\n    from crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n    \n    async def main():\n        async with AsyncWebCrawler(verbose=True) as crawler:\n            url = \"https://github.com/microsoft/TypeScript/commits/main\"\n            session_id = \"typescript_commits_session\"\n            all_commits = []\n    \n            js_next_page = \"\"\"\n            const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n            if (button) button.click();\n            \"\"\"\n    \n            wait_for = \"\"\"() => {\n                const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n                if (commits.length === 0) return false;\n                const firstCommit = commits[0].textContent.trim();\n                return firstCommit !== window.lastCommit;\n            }\"\"\"\n    \n            schema = {\n                \"name\": \"Commit Extractor\",\n                \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n                \"fields\": [\n                    {\n                        \"name\": \"title\",\n                        \"selector\": \"h4.markdown-title\",\n                        \"type\": \"text\",\n                        \"transform\": \"strip\",\n                    },\n                ],\n            }\n            extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n    \n            for page in range(3):  # Crawl 3 pages\n                result = await crawler.arun(\n                    url=url,\n                    session_id=session_id,\n                    css_selector=\"li.Box-sc-g0xbh4-0\",\n                    extraction_strategy=extraction_strategy,\n                    js_code=js_next_page if page > 0 else None,\n                    wait_for=wait_for if page > 0 else None,\n                    js_only=page > 0,\n                    bypass_cache=True,\n                    headless=False,\n                )\n    \n                assert result.success, f\"Failed to crawl page {page + 1}\"\n    \n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n    \n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n    \n            await crawler.crawler_strategy.kill_session(session_id)\n            print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n    \n    asyncio.run(main())\n    \n\nIn this example, we're crawling multiple pages of commits from a GitHub\nrepository. The URL doesn't change as we load more commits, so we use\nJavaScript to click the \"Load More\" button and a `wait_for` condition to\nensure the new content is loaded before extraction. This powerful combination\nallows us to navigate and extract data from complex, dynamically-loaded web\napplications with ease!\n\n## Congratulations! ðŸŽ‰\n\nYou've made it through the Crawl4AI Quickstart Guide! Now go forth and crawl\nthe web asynchronously like a pro! ðŸ•¸ï¸\n\nRemember, these are just a few examples of what Crawl4AI can do. For more\nadvanced usage, check out our other documentation pages:\n\n  * [LLM Extraction](../examples/llm_extraction/)\n  * [JS Execution & CSS Filtering](../examples/js_execution_css_filtering/)\n  * [Hooks & Auth](../examples/hooks_auth/)\n  * [Summarization](../examples/summarization/)\n  * [Research Assistant](../examples/research_assistant/)\n\nHappy crawling! ðŸš€\n\n"
    },
    {
      "title": "Intro",
      "url": "https://crawl4ai.com/mkdocs/examples/",
      "content": "# Examples\n\nWelcome to the examples section of Crawl4AI documentation! In this section,\nyou will find practical examples demonstrating how to use Crawl4AI for various\nweb crawling and data extraction tasks. Each example is designed to showcase\ndifferent features and capabilities of the library.\n\n## Examples Index\n\n### [LLM Extraction](llm_extraction/)\n\nThis example demonstrates how to use Crawl4AI to extract information using\nLarge Language Models (LLMs). You will learn how to configure the\n`LLMExtractionStrategy` to get structured data from web pages.\n\n### [JSON CSS Extraction](json_css_extraction/)\n\nThis example demonstrates how to use Crawl4AI to extract structured data\nwithout using LLM, and just focusing on page structure. You will learn how to\nuse the `JsonCssExtractionStrategy` to extract data using CSS selectors.\n\n### [JS Execution & CSS Filtering](js_execution_css_filtering/)\n\nLearn how to execute custom JavaScript code and filter data using CSS\nselectors. This example shows how to perform complex web interactions and\nextract specific content from web pages.\n\n### [Hooks & Auth](hooks_auth/)\n\nThis example covers the use of custom hooks for authentication and other pre-\ncrawling tasks. You will see how to set up hooks to modify headers,\nauthenticate sessions, and perform other preparatory actions before crawling.\n\n### [Summarization](summarization/)\n\nDiscover how to use Crawl4AI to summarize web page content. This example\ndemonstrates the summarization capabilities of the library, helping you\nextract concise information from lengthy web pages.\n\n### [Research Assistant](research_assistant/)\n\nIn this example, Crawl4AI is used as a research assistant to gather and\norganize information from multiple sources. You will learn how to use various\nextraction and chunking strategies to compile a comprehensive report.\n\n* * *\n\nEach example includes detailed explanations and code snippets to help you\nunderstand and implement the features in your projects. Click on the links to\nexplore each example and start making the most of Crawl4AI!\n\n"
    },
    {
      "title": "Structured Data Extraction",
      "url": "https://crawl4ai.com/mkdocs/examples/json_css_extraction/",
      "content": "# JSON CSS Extraction Strategy with AsyncWebCrawler\n\nThe `JsonCssExtractionStrategy` is a powerful feature of Crawl4AI that allows\nyou to extract structured data from web pages using CSS selectors. This method\nis particularly useful when you need to extract specific data points from a\nconsistent HTML structure, such as tables or repeated elements. Here's how to\nuse it with the AsyncWebCrawler.\n\n## Overview\n\nThe `JsonCssExtractionStrategy` works by defining a schema that specifies: 1\\.\nA base CSS selector for the repeating elements 2\\. Fields to extract from each\nelement, each with its own CSS selector\n\nThis strategy is fast and efficient, as it doesn't rely on external services\nlike LLMs for extraction.\n\n## Example: Extracting Cryptocurrency Prices from Coinbase\n\nLet's look at an example that extracts cryptocurrency prices from the Coinbase\nexplore page.\n\n    \n    \n    import json\n    import asyncio\n    from crawl4ai import AsyncWebCrawler\n    from crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n    \n    async def extract_structured_data_using_css_extractor():\n        print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    \n        # Define the extraction schema\n        schema = {\n            \"name\": \"Coinbase Crypto Prices\",\n            \"baseSelector\": \".cds-tableRow-t45thuk\",\n            \"fields\": [\n                {\n                    \"name\": \"crypto\",\n                    \"selector\": \"td:nth-child(1) h2\",\n                    \"type\": \"text\",\n                },\n                {\n                    \"name\": \"symbol\",\n                    \"selector\": \"td:nth-child(1) p\",\n                    \"type\": \"text\",\n                },\n                {\n                    \"name\": \"price\",\n                    \"selector\": \"td:nth-child(2)\",\n                    \"type\": \"text\",\n                }\n            ],\n        }\n    \n        # Create the extraction strategy\n        extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n    \n        # Use the AsyncWebCrawler with the extraction strategy\n        async with AsyncWebCrawler(verbose=True) as crawler:\n            result = await crawler.arun(\n                url=\"https://www.coinbase.com/explore\",\n                extraction_strategy=extraction_strategy,\n                bypass_cache=True,\n            )\n    \n            assert result.success, \"Failed to crawl the page\"\n    \n            # Parse the extracted content\n            crypto_prices = json.loads(result.extracted_content)\n            print(f\"Successfully extracted {len(crypto_prices)} cryptocurrency prices\")\n            print(json.dumps(crypto_prices[0], indent=2))\n    \n        return crypto_prices\n    \n    # Run the async function\n    asyncio.run(extract_structured_data_using_css_extractor())\n    \n\n## Explanation of the Schema\n\nThe schema defines how to extract the data:\n\n  * `name`: A descriptive name for the extraction task.\n  * `baseSelector`: The CSS selector for the repeating elements (in this case, table rows).\n  * `fields`: An array of fields to extract from each element:\n  * `name`: The name to give the extracted data.\n  * `selector`: The CSS selector to find the specific data within the base element.\n  * `type`: The type of data to extract (usually \"text\" for textual content).\n\n## Advantages of JsonCssExtractionStrategy\n\n  1. **Speed** : CSS selectors are fast to execute, making this method efficient for large datasets.\n  2. **Precision** : You can target exactly the elements you need.\n  3. **Structured Output** : The result is already structured as JSON, ready for further processing.\n  4. **No External Dependencies** : Unlike LLM-based strategies, this doesn't require any API calls to external services.\n\n## Tips for Using JsonCssExtractionStrategy\n\n  1. **Inspect the Page** : Use browser developer tools to identify the correct CSS selectors.\n  2. **Test Selectors** : Verify your selectors in the browser console before using them in the script.\n  3. **Handle Dynamic Content** : If the page uses JavaScript to load content, you may need to combine this with JS execution (see the Advanced Usage section).\n  4. **Error Handling** : Always check the `result.success` flag and handle potential failures.\n\n## Advanced Usage: Combining with JavaScript Execution\n\nFor pages that load data dynamically, you can combine the\n`JsonCssExtractionStrategy` with JavaScript execution:\n\n    \n    \n    async def extract_dynamic_structured_data():\n        schema = {\n            \"name\": \"Dynamic Crypto Prices\",\n            \"baseSelector\": \".crypto-row\",\n            \"fields\": [\n                {\"name\": \"name\", \"selector\": \".crypto-name\", \"type\": \"text\"},\n                {\"name\": \"price\", \"selector\": \".crypto-price\", \"type\": \"text\"},\n            ]\n        }\n    \n        js_code = \"\"\"\n        window.scrollTo(0, document.body.scrollHeight);\n        await new Promise(resolve => setTimeout(resolve, 2000));  // Wait for 2 seconds\n        \"\"\"\n    \n        extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n    \n        async with AsyncWebCrawler(verbose=True) as crawler:\n            result = await crawler.arun(\n                url=\"https://example.com/crypto-prices\",\n                extraction_strategy=extraction_strategy,\n                js_code=js_code,\n                wait_for=\".crypto-row:nth-child(20)\",  # Wait for 20 rows to load\n                bypass_cache=True,\n            )\n    \n            crypto_data = json.loads(result.extracted_content)\n            print(f\"Extracted {len(crypto_data)} cryptocurrency entries\")\n    \n    asyncio.run(extract_dynamic_structured_data())\n    \n\nThis advanced example demonstrates how to: 1\\. Execute JavaScript to trigger\ndynamic content loading. 2\\. Wait for a specific condition (20 rows loaded)\nbefore extraction. 3\\. Extract data from the dynamically loaded content.\n\nBy mastering the `JsonCssExtractionStrategy`, you can efficiently extract\nstructured data from a wide variety of web pages, making it a valuable tool in\nyour web scraping toolkit.\n\nFor more details on schema definitions and advanced extraction strategies,\ncheck out the[Advanced\nJsonCssExtraction](../../full_details/advanced_jsoncss_extraction/).\n\n"
    },
    {
      "title": "LLM Extraction",
      "url": "https://crawl4ai.com/mkdocs/examples/llm_extraction/",
      "content": "# LLM Extraction with AsyncWebCrawler\n\nCrawl4AI's AsyncWebCrawler allows you to use Language Models (LLMs) to extract\nstructured data or relevant content from web pages asynchronously. Below are\ntwo examples demonstrating how to use `LLMExtractionStrategy` for different\npurposes with the AsyncWebCrawler.\n\n## Example 1: Extract Structured Data\n\nIn this example, we use the `LLMExtractionStrategy` to extract structured data\n(model names and their fees) from the OpenAI pricing page.\n\n    \n    \n    import os\n    import json\n    import asyncio\n    from crawl4ai import AsyncWebCrawler\n    from crawl4ai.extraction_strategy import LLMExtractionStrategy\n    from pydantic import BaseModel, Field\n    \n    class OpenAIModelFee(BaseModel):\n        model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n        input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n        output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n    \n    async def extract_openai_fees():\n        url = 'https://openai.com/api/pricing/'\n    \n        async with AsyncWebCrawler(verbose=True) as crawler:\n            result = await crawler.arun(\n                url=url,\n                word_count_threshold=1,\n                extraction_strategy=LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    schema=OpenAIModelFee.model_json_schema(),\n                    extraction_type=\"schema\",\n                    instruction=\"From the crawled content, extract all mentioned model names along with their \"\n                                \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n                                'One extracted model JSON format should look like this: '\n                                '{ \"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\" }'\n                ),\n                bypass_cache=True,\n            )\n    \n        model_fees = json.loads(result.extracted_content)\n        print(f\"Number of models extracted: {len(model_fees)}\")\n    \n        with open(\".data/openai_fees.json\", \"w\", encoding=\"utf-8\") as f:\n            json.dump(model_fees, f, indent=2)\n    \n    asyncio.run(extract_openai_fees())\n    \n\n## Example 2: Extract Relevant Content\n\nIn this example, we instruct the LLM to extract only content related to\ntechnology from the NBC News business page.\n\n    \n    \n    import os\n    import json\n    import asyncio\n    from crawl4ai import AsyncWebCrawler\n    from crawl4ai.extraction_strategy import LLMExtractionStrategy\n    \n    async def extract_tech_content():\n        async with AsyncWebCrawler(verbose=True) as crawler:\n            result = await crawler.arun(\n                url=\"https://www.nbcnews.com/business\",\n                extraction_strategy=LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Extract only content related to technology\"\n                ),\n                bypass_cache=True,\n            )\n    \n        tech_content = json.loads(result.extracted_content)\n        print(f\"Number of tech-related items extracted: {len(tech_content)}\")\n    \n        with open(\".data/tech_content.json\", \"w\", encoding=\"utf-8\") as f:\n            json.dump(tech_content, f, indent=2)\n    \n    asyncio.run(extract_tech_content())\n    \n\n## Advanced Usage: Combining JS Execution with LLM Extraction\n\nThis example demonstrates how to combine JavaScript execution with LLM\nextraction to handle dynamic content:\n\n    \n    \n    async def extract_dynamic_content():\n        js_code = \"\"\"\n        const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n        if (loadMoreButton) {\n            loadMoreButton.click();\n            await new Promise(resolve => setTimeout(resolve, 2000));\n        }\n        \"\"\"\n    \n        wait_for = \"\"\"\n        () => {\n            const articles = document.querySelectorAll('article.tease-card');\n            return articles.length > 10;\n        }\n        \"\"\"\n    \n        async with AsyncWebCrawler(verbose=True) as crawler:\n            result = await crawler.arun(\n                url=\"https://www.nbcnews.com/business\",\n                js_code=js_code,\n                wait_for=wait_for,\n                css_selector=\"article.tease-card\",\n                extraction_strategy=LLMExtractionStrategy(\n                    provider=\"openai/gpt-4o\",\n                    api_token=os.getenv('OPENAI_API_KEY'),\n                    instruction=\"Summarize each article, focusing on technology-related content\"\n                ),\n                bypass_cache=True,\n            )\n    \n        summaries = json.loads(result.extracted_content)\n        print(f\"Number of summarized articles: {len(summaries)}\")\n    \n        with open(\".data/tech_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n            json.dump(summaries, f, indent=2)\n    \n    asyncio.run(extract_dynamic_content())\n    \n\n## Customizing LLM Provider\n\nCrawl4AI uses the `litellm` library under the hood, which allows you to use\nany LLM provider you want. Just pass the correct model name and API token:\n\n    \n    \n    extraction_strategy=LLMExtractionStrategy(\n        provider=\"your_llm_provider/model_name\",\n        api_token=\"your_api_token\",\n        instruction=\"Your extraction instruction\"\n    )\n    \n\nThis flexibility allows you to integrate with various LLM providers and tailor\nthe extraction process to your specific needs.\n\n## Error Handling and Retries\n\nWhen working with external LLM APIs, it's important to handle potential errors\nand implement retry logic. Here's an example of how you might do this:\n\n    \n    \n    import asyncio\n    from tenacity import retry, stop_after_attempt, wait_exponential\n    \n    class LLMExtractionError(Exception):\n        pass\n    \n    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\n    async def extract_with_retry(crawler, url, extraction_strategy):\n        try:\n            result = await crawler.arun(url=url, extraction_strategy=extraction_strategy, bypass_cache=True)\n            return json.loads(result.extracted_content)\n        except Exception as e:\n            raise LLMExtractionError(f\"Failed to extract content: {str(e)}\")\n    \n    async def main():\n        async with AsyncWebCrawler(verbose=True) as crawler:\n            try:\n                content = await extract_with_retry(\n                    crawler,\n                    \"https://www.example.com\",\n                    LLMExtractionStrategy(\n                        provider=\"openai/gpt-4o\",\n                        api_token=os.getenv('OPENAI_API_KEY'),\n                        instruction=\"Extract and summarize main points\"\n                    )\n                )\n                print(\"Extracted content:\", content)\n            except LLMExtractionError as e:\n                print(f\"Extraction failed after retries: {e}\")\n    \n    asyncio.run(main())\n    \n\nThis example uses the `tenacity` library to implement a retry mechanism with\nexponential backoff, which can help handle temporary failures or rate limiting\nfrom the LLM API.\n\n"
    },
    {
      "title": "JS Execution & CSS Filtering",
      "url": "https://crawl4ai.com/mkdocs/examples/js_execution_css_filtering/",
      "content": "# JS Execution & CSS Filtering with AsyncWebCrawler\n\nIn this example, we'll demonstrate how to use Crawl4AI's AsyncWebCrawler to\nexecute JavaScript, filter data with CSS selectors, and use a cosine\nsimilarity strategy to extract relevant content. This approach is particularly\nuseful when you need to interact with dynamic content on web pages, such as\nclicking \"Load More\" buttons.\n\n## Example: Extracting Structured Data Asynchronously\n\n    \n    \n    import asyncio\n    from crawl4ai import AsyncWebCrawler\n    from crawl4ai.chunking_strategy import RegexChunking\n    from crawl4ai.extraction_strategy import CosineStrategy\n    from crawl4ai.async_crawler_strategy import AsyncPlaywrightCrawlerStrategy\n    \n    async def main():\n        # Define the JavaScript code to click the \"Load More\" button\n        js_code = \"\"\"\n        const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));\n        if (loadMoreButton) {\n            loadMoreButton.click();\n            // Wait for new content to load\n            await new Promise(resolve => setTimeout(resolve, 2000));\n        }\n        \"\"\"\n    \n        # Define a wait_for function to ensure content is loaded\n        wait_for = \"\"\"\n        () => {\n            const articles = document.querySelectorAll('article.tease-card');\n            return articles.length > 10;\n        }\n        \"\"\"\n    \n        async with AsyncWebCrawler(verbose=True) as crawler:\n            # Run the crawler with keyword filtering and CSS selector\n            result = await crawler.arun(\n                url=\"https://www.nbcnews.com/business\",\n                js_code=js_code,\n                wait_for=wait_for,\n                css_selector=\"article.tease-card\",\n                extraction_strategy=CosineStrategy(\n                    semantic_filter=\"technology\",\n                ),\n                chunking_strategy=RegexChunking(),\n            )\n    \n        # Display the extracted result\n        print(result.extracted_content)\n    \n    # Run the async function\n    asyncio.run(main())\n    \n\n### Explanation\n\n  1. **Asynchronous Execution** : We use `AsyncWebCrawler` with async/await syntax for non-blocking execution.\n\n  2. **JavaScript Execution** : The `js_code` variable contains JavaScript code that simulates clicking a \"Load More\" button and waits for new content to load.\n\n  3. **Wait Condition** : The `wait_for` function ensures that the page has loaded more than 10 articles before proceeding with the extraction.\n\n  4. **CSS Selector** : The `css_selector=\"article.tease-card\"` parameter ensures that only article cards are extracted from the web page.\n\n  5. **Extraction Strategy** : The `CosineStrategy` is used with a semantic filter for \"technology\" to extract relevant content based on cosine similarity.\n\n  6. **Chunking Strategy** : We use `RegexChunking()` to split the content into manageable chunks for processing.\n\n## Advanced Usage: Custom Session and Multiple Requests\n\nFor more complex scenarios where you need to maintain state across multiple\nrequests or execute additional JavaScript after the initial page load, you can\nuse a custom session:\n\n    \n    \n    async def advanced_crawl():\n        async with AsyncWebCrawler(verbose=True) as crawler:\n            # Initial crawl with custom session\n            result1 = await crawler.arun(\n                url=\"https://www.nbcnews.com/business\",\n                js_code=js_code,\n                wait_for=wait_for,\n                css_selector=\"article.tease-card\",\n                session_id=\"business_session\"\n            )\n    \n            # Execute additional JavaScript in the same session\n            result2 = await crawler.crawler_strategy.execute_js(\n                session_id=\"business_session\",\n                js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n                wait_for_js=\"() => window.innerHeight + window.scrollY >= document.body.offsetHeight\"\n            )\n    \n            # Process results\n            print(\"Initial crawl result:\", result1.extracted_content)\n            print(\"Additional JS execution result:\", result2.html)\n    \n    asyncio.run(advanced_crawl())\n    \n\nThis advanced example demonstrates how to: 1\\. Use a custom session to\nmaintain state across requests. 2\\. Execute additional JavaScript after the\ninitial page load. 3\\. Wait for specific conditions using JavaScript\nfunctions.\n\n## Try It Yourself\n\nThese examples demonstrate the power and flexibility of Crawl4AI's\nAsyncWebCrawler in handling complex web interactions and extracting meaningful\ndata asynchronously. You can customize the JavaScript code, CSS selectors,\nextraction strategies, and waiting conditions to suit your specific\nrequirements.\n\n"
    },
    {
      "title": "Hooks & Auth",
      "url": "https://crawl4ai.com/mkdocs/examples/hooks_auth/",
      "content": "# Hooks & Auth for AsyncWebCrawler\n\nCrawl4AI's AsyncWebCrawler allows you to customize the behavior of the web\ncrawler using hooks. Hooks are asynchronous functions that are called at\nspecific points in the crawling process, allowing you to modify the crawler's\nbehavior or perform additional actions. This example demonstrates how to use\nvarious hooks to customize the asynchronous crawling process.\n\n## Example: Using Crawler Hooks with AsyncWebCrawler\n\nLet's see how we can customize the AsyncWebCrawler using hooks! In this\nexample, we'll:\n\n  1. Configure the browser when it's created.\n  2. Add custom headers before navigating to the URL.\n  3. Log the current URL after navigation.\n  4. Perform actions after JavaScript execution.\n  5. Log the length of the HTML before returning it.\n\n### Hook Definitions\n\n    \n    \n    import asyncio\n    from crawl4ai import AsyncWebCrawler\n    from crawl4ai.async_crawler_strategy import AsyncPlaywrightCrawlerStrategy\n    from playwright.async_api import Page, Browser\n    \n    async def on_browser_created(browser: Browser):\n        print(\"[HOOK] on_browser_created\")\n        # Example customization: set browser viewport size\n        context = await browser.new_context(viewport={'width': 1920, 'height': 1080})\n        page = await context.new_page()\n    \n        # Example customization: logging in to a hypothetical website\n        await page.goto('https://example.com/login')\n        await page.fill('input[name=\"username\"]', 'testuser')\n        await page.fill('input[name=\"password\"]', 'password123')\n        await page.click('button[type=\"submit\"]')\n        await page.wait_for_selector('#welcome')\n    \n        # Add a custom cookie\n        await context.add_cookies([{'name': 'test_cookie', 'value': 'cookie_value', 'url': 'https://example.com'}])\n    \n        await page.close()\n        await context.close()\n    \n    async def before_goto(page: Page):\n        print(\"[HOOK] before_goto\")\n        # Example customization: add custom headers\n        await page.set_extra_http_headers({'X-Test-Header': 'test'})\n    \n    async def after_goto(page: Page):\n        print(\"[HOOK] after_goto\")\n        # Example customization: log the URL\n        print(f\"Current URL: {page.url}\")\n    \n    async def on_execution_started(page: Page):\n        print(\"[HOOK] on_execution_started\")\n        # Example customization: perform actions after JS execution\n        await page.evaluate(\"console.log('Custom JS executed')\")\n    \n    async def before_return_html(page: Page, html: str):\n        print(\"[HOOK] before_return_html\")\n        # Example customization: log the HTML length\n        print(f\"HTML length: {len(html)}\")\n        return page\n    \n\n### Using the Hooks with the AsyncWebCrawler\n\n    \n    \n    import asyncio\n    from crawl4ai import AsyncWebCrawler\n    from crawl4ai.async_crawler_strategy import AsyncPlaywrightCrawlerStrategy\n    \n    async def main():\n        print(\"\\nðŸ”— Using Crawler Hooks: Let's see how we can customize the AsyncWebCrawler using hooks!\")\n    \n        crawler_strategy = AsyncPlaywrightCrawlerStrategy(verbose=True)\n        crawler_strategy.set_hook('on_browser_created', on_browser_created)\n        crawler_strategy.set_hook('before_goto', before_goto)\n        crawler_strategy.set_hook('after_goto', after_goto)\n        crawler_strategy.set_hook('on_execution_started', on_execution_started)\n        crawler_strategy.set_hook('before_return_html', before_return_html)\n    \n        async with AsyncWebCrawler(verbose=True, crawler_strategy=crawler_strategy) as crawler:\n            result = await crawler.arun(\n                url=\"https://example.com\",\n                js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n                wait_for=\"footer\"\n            )\n    \n        print(\"ðŸ“¦ Crawler Hooks result:\")\n        print(result)\n    \n    asyncio.run(main())\n    \n\n### Explanation\n\n  * `on_browser_created`: This hook is called when the Playwright browser is created. It sets up the browser context, logs in to a website, and adds a custom cookie.\n  * `before_goto`: This hook is called right before Playwright navigates to the URL. It adds custom HTTP headers.\n  * `after_goto`: This hook is called after Playwright navigates to the URL. It logs the current URL.\n  * `on_execution_started`: This hook is called after any custom JavaScript is executed. It performs additional JavaScript actions.\n  * `before_return_html`: This hook is called before returning the HTML content. It logs the length of the HTML content.\n\n### Additional Ideas\n\n  * **Handling authentication** : Use the `on_browser_created` hook to handle login processes or set authentication tokens.\n  * **Dynamic header modification** : Modify headers based on the target URL or other conditions in the `before_goto` hook.\n  * **Content verification** : Use the `after_goto` hook to verify that the expected content is present on the page.\n  * **Custom JavaScript injection** : Inject and execute custom JavaScript using the `on_execution_started` hook.\n  * **Content preprocessing** : Modify or analyze the HTML content in the `before_return_html` hook before it's returned.\n\nBy using these hooks, you can customize the behavior of the AsyncWebCrawler to\nsuit your specific needs, including handling authentication, modifying\nrequests, and preprocessing content.\n\n"
    },
    {
      "title": "Summarization",
      "url": "https://crawl4ai.com/mkdocs/examples/summarization/",
      "content": "# Summarization Example with AsyncWebCrawler\n\nThis example demonstrates how to use Crawl4AI's `AsyncWebCrawler` to extract a\nsummary from a web page asynchronously. The goal is to obtain the title, a\ndetailed summary, a brief summary, and a list of keywords from the given page.\n\n## Step-by-Step Guide\n\n  1. **Import Necessary Modules**\n\nFirst, import the necessary modules and classes:\n\n`python import os import json import asyncio from crawl4ai import\nAsyncWebCrawler from crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom crawl4ai.chunking_strategy import RegexChunking from pydantic import\nBaseModel, Field`\n\n  2. **Define the URL to be Crawled**\n\nSet the URL of the web page you want to summarize:\n\n`python url =\n'https://marketplace.visualstudio.com/items?itemName=Unclecode.groqopilot'`\n\n  3. **Define the Data Model**\n\nUse Pydantic to define the structure of the extracted data:\n\n`python class PageSummary(BaseModel): title: str = Field(...,\ndescription=\"Title of the page.\") summary: str = Field(...,\ndescription=\"Summary of the page.\") brief_summary: str = Field(...,\ndescription=\"Brief summary of the page.\") keywords: list = Field(...,\ndescription=\"Keywords assigned to the page.\")`\n\n  4. **Create the Extraction Strategy**\n\nSet up the `LLMExtractionStrategy` with the necessary parameters:\n\n`python extraction_strategy = LLMExtractionStrategy( provider=\"openai/gpt-4o\",\napi_token=os.getenv('OPENAI_API_KEY'), schema=PageSummary.model_json_schema(),\nextraction_type=\"schema\", apply_chunking=False, instruction=( \"From the\ncrawled content, extract the following details: \" \"1. Title of the page \" \"2.\nSummary of the page, which is a detailed summary \" \"3. Brief summary of the\npage, which is a paragraph text \" \"4. Keywords assigned to the page, which is\na list of keywords. \" 'The extracted JSON format should look like this: ' '{\n\"title\": \"Page Title\", \"summary\": \"Detailed summary of the page.\", '\n'\"brief_summary\": \"Brief summary in a paragraph.\", \"keywords\": [\"keyword1\",\n\"keyword2\", \"keyword3\"] }' ) )`\n\n  5. **Define the Async Crawl Function**\n\nCreate an asynchronous function to run the crawler:\n\n`python async def crawl_and_summarize(url): async with\nAsyncWebCrawler(verbose=True) as crawler: result = await crawler.arun(\nurl=url, word_count_threshold=1, extraction_strategy=extraction_strategy,\nchunking_strategy=RegexChunking(), bypass_cache=True, ) return result`\n\n  6. **Run the Crawler and Process Results**\n\nUse asyncio to run the crawler and process the results:\n\n```python async def main(): result = await crawl_and_summarize(url)\n\n    \n        if result.success:\n        page_summary = json.loads(result.extracted_content)\n        print(\"Extracted Page Summary:\")\n        print(json.dumps(page_summary, indent=2))\n    \n        # Save the extracted data\n        with open(\".data/page_summary.json\", \"w\", encoding=\"utf-8\") as f:\n            json.dump(page_summary, f, indent=2)\n        print(\"Page summary saved to .data/page_summary.json\")\n    else:\n        print(f\"Failed to crawl and summarize the page. Error: {result.error_message}\")\n    \n\n# Run the async main function\n\nasyncio.run(main()) ```\n\n## Explanation\n\n  * **Importing Modules** : We import the necessary modules, including `AsyncWebCrawler` and `LLMExtractionStrategy` from Crawl4AI.\n  * **URL Definition** : We set the URL of the web page to crawl and summarize.\n  * **Data Model Definition** : We define the structure of the data to extract using Pydantic's `BaseModel`.\n  * **Extraction Strategy Setup** : We create an instance of `LLMExtractionStrategy` with the schema and detailed instructions for the extraction process.\n  * **Async Crawl Function** : We define an asynchronous function `crawl_and_summarize` that uses `AsyncWebCrawler` to perform the crawling and extraction.\n  * **Main Execution** : In the `main` function, we run the crawler, process the results, and save the extracted data.\n\n## Advanced Usage: Crawling Multiple URLs\n\nTo demonstrate the power of `AsyncWebCrawler`, here's how you can summarize\nmultiple pages concurrently:\n\n    \n    \n    async def crawl_multiple_urls(urls):\n        async with AsyncWebCrawler(verbose=True) as crawler:\n            tasks = [crawler.arun(\n                url=url,\n                word_count_threshold=1,\n                extraction_strategy=extraction_strategy,\n                chunking_strategy=RegexChunking(),\n                bypass_cache=True\n            ) for url in urls]\n            results = await asyncio.gather(*tasks)\n        return results\n    \n    async def main():\n        urls = [\n            'https://marketplace.visualstudio.com/items?itemName=Unclecode.groqopilot',\n            'https://marketplace.visualstudio.com/items?itemName=GitHub.copilot',\n            'https://marketplace.visualstudio.com/items?itemName=ms-python.python'\n        ]\n        results = await crawl_multiple_urls(urls)\n    \n        for i, result in enumerate(results):\n            if result.success:\n                page_summary = json.loads(result.extracted_content)\n                print(f\"\\nSummary for URL {i+1}:\")\n                print(json.dumps(page_summary, indent=2))\n            else:\n                print(f\"\\nFailed to summarize URL {i+1}. Error: {result.error_message}\")\n    \n    asyncio.run(main())\n    \n\nThis advanced example shows how to use `AsyncWebCrawler` to efficiently\nsummarize multiple web pages concurrently, significantly reducing the total\nprocessing time compared to sequential crawling.\n\nBy leveraging the asynchronous capabilities of Crawl4AI, you can perform\nadvanced web crawling and data extraction tasks with improved efficiency and\nscalability.\n\n"
    },
    {
      "title": "Research Assistant",
      "url": "https://crawl4ai.com/mkdocs/examples/research_assistant/",
      "content": "# Research Assistant Example with AsyncWebCrawler\n\nThis example demonstrates how to build an advanced research assistant using\n`Chainlit`, `Crawl4AI`'s `AsyncWebCrawler`, and various AI services. The\nassistant can crawl web pages asynchronously, answer questions based on the\ncrawled content, and handle audio inputs.\n\n## Step-by-Step Guide\n\n  1. **Install Required Packages**\n\nEnsure you have the necessary packages installed:\n\n`bash pip install chainlit groq openai crawl4ai`\n\n  2. **Import Libraries**\n\n```python import os import time import asyncio from openai import AsyncOpenAI\nimport chainlit as cl import re from io import BytesIO from chainlit.element\nimport ElementBased from groq import Groq from crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extraction_strategy import NoExtractionStrategy from\ncrawl4ai.chunking_strategy import RegexChunking\n\nclient = AsyncOpenAI(base_url=\"https://api.groq.com/openai/v1\",\napi_key=os.getenv(\"GROQ_API_KEY\"))\n\n# Instrument the OpenAI client\n\ncl.instrument_openai() ```\n\n  3. **Set Configuration**\n\n`python settings = { \"model\": \"llama3-8b-8192\", \"temperature\": 0.5,\n\"max_tokens\": 500, \"top_p\": 1, \"frequency_penalty\": 0, \"presence_penalty\": 0,\n}`\n\n  4. **Define Utility Functions**\n\n```python def extract_urls(text): url_pattern = re.compile(r'(https?://\\S+)')\nreturn url_pattern.findall(text)\n\nasync def crawl_urls(urls): async with AsyncWebCrawler(verbose=True) as\ncrawler: results = await crawler.arun_many( urls=urls,\nword_count_threshold=10, extraction_strategy=NoExtractionStrategy(),\nchunking_strategy=RegexChunking(), bypass_cache=True ) return [result.markdown\nfor result in results if result.success] ```\n\n  5. **Initialize Chat Start Event**\n\n`python @cl.on_chat_start async def on_chat_start():\ncl.user_session.set(\"session\", { \"history\": [], \"context\": {} }) await\ncl.Message(content=\"Welcome to the chat! How can I assist you today?\").send()`\n\n  6. **Handle Incoming Messages**\n\n```python @cl.on_message async def on_message(message: cl.Message):\nuser_session = cl.user_session.get(\"session\")\n\n    \n        # Extract URLs from the user's message\n    urls = extract_urls(message.content)\n    \n    if urls:\n        crawled_contents = await crawl_urls(urls)\n        for url, content in zip(urls, crawled_contents):\n            ref_number = f\"REF_{len(user_session['context']) + 1}\"\n            user_session[\"context\"][ref_number] = {\n                \"url\": url,\n                \"content\": content\n            }\n    \n    user_session[\"history\"].append({\n        \"role\": \"user\",\n        \"content\": message.content\n    })\n    \n    # Create a system message that includes the context\n    context_messages = [\n        f'<appendix ref=\"{ref}\">\\n{data[\"content\"]}\\n</appendix>'\n        for ref, data in user_session[\"context\"].items()\n    ]\n    system_message = {\n        \"role\": \"system\",\n        \"content\": (\n            \"You are a helpful bot. Use the following context for answering questions. \"\n            \"Refer to the sources using the REF number in square brackets, e.g., [1], only if the source is given in the appendices below.\\n\\n\"\n            \"If the question requires any information from the provided appendices or context, refer to the sources. \"\n            \"If not, there is no need to add a references section. \"\n            \"At the end of your response, provide a reference section listing the URLs and their REF numbers only if sources from the appendices were used.\\n\\n\"\n            \"\\n\\n\".join(context_messages)\n        ) if context_messages else \"You are a helpful assistant.\"\n    }\n    \n    msg = cl.Message(content=\"\")\n    await msg.send()\n    \n    # Get response from the LLM\n    stream = await client.chat.completions.create(\n        messages=[system_message, *user_session[\"history\"]],\n        stream=True,\n        **settings\n    )\n    \n    assistant_response = \"\"\n    async for part in stream:\n        if token := part.choices[0].delta.content:\n            assistant_response += token\n            await msg.stream_token(token)\n    \n    # Add assistant message to the history\n    user_session[\"history\"].append({\n        \"role\": \"assistant\",\n        \"content\": assistant_response\n    })\n    await msg.update()\n    \n    # Append the reference section to the assistant's response\n    if user_session[\"context\"]:\n        reference_section = \"\\n\\nReferences:\\n\"\n        for ref, data in user_session[\"context\"].items():\n            reference_section += f\"[{ref.split('_')[1]}]: {data['url']}\\n\"\n        msg.content += reference_section\n        await msg.update()\n    \n\n```\n\n  7. **Handle Audio Input**\n\n```python @cl.on_audio_chunk async def on_audio_chunk(chunk: cl.AudioChunk):\nif chunk.isStart: buffer = BytesIO() buffer.name =\nf\"input_audio.{chunk.mimeType.split('/')[1]}\"\ncl.user_session.set(\"audio_buffer\", buffer)\ncl.user_session.set(\"audio_mime_type\", chunk.mimeType)\ncl.user_session.get(\"audio_buffer\").write(chunk.data)\n\n@cl.step(type=\"tool\") async def speech_to_text(audio_file): response = await\nclient.audio.transcriptions.create( model=\"whisper-large-v3\", file=audio_file\n) return response.text\n\n@cl.on_audio_end async def on_audio_end(elements: list[ElementBased]):\naudio_buffer: BytesIO = cl.user_session.get(\"audio_buffer\")\naudio_buffer.seek(0) audio_file = audio_buffer.read() audio_mime_type: str =\ncl.user_session.get(\"audio_mime_type\")\n\n    \n        start_time = time.time()\n    transcription = await speech_to_text((audio_buffer.name, audio_file, audio_mime_type))\n    end_time = time.time()\n    print(f\"Transcription took {end_time - start_time} seconds\")\n    \n    user_msg = cl.Message(author=\"You\", type=\"user_message\", content=transcription)\n    await user_msg.send()\n    await on_message(user_msg)\n    \n\n```\n\n  8. **Run the Chat Application**\n\n`python if __name__ == \"__main__\": from chainlit.cli import run_chainlit\nrun_chainlit(__file__)`\n\n## Explanation\n\n  * **Libraries and Configuration** : We import necessary libraries, including `AsyncWebCrawler` from `crawl4ai`.\n  * **Utility Functions** : \n  * `extract_urls`: Uses regex to find URLs in messages.\n  * `crawl_urls`: An asynchronous function that uses `AsyncWebCrawler` to fetch content from multiple URLs concurrently.\n  * **Chat Start Event** : Initializes the chat session and sends a welcome message.\n  * **Message Handling** : \n  * Extracts URLs from user messages.\n  * Asynchronously crawls the URLs using `AsyncWebCrawler`.\n  * Updates chat history and context with crawled content.\n  * Generates a response using the LLM, incorporating the crawled context.\n  * **Audio Handling** : Captures, buffers, and transcribes audio input, then processes the transcription as text.\n  * **Running the Application** : Starts the Chainlit server for interaction with the assistant.\n\n## Key Improvements\n\n  1. **Asynchronous Web Crawling** : Using `AsyncWebCrawler` allows for efficient, concurrent crawling of multiple URLs.\n  2. **Improved Context Management** : The assistant now maintains a context of crawled content, allowing for more informed responses.\n  3. **Dynamic Reference System** : The assistant can refer to specific sources in its responses and provide a reference section.\n  4. **Seamless Audio Integration** : The ability to handle audio inputs makes the assistant more versatile and user-friendly.\n\nThis updated Research Assistant showcases how to create a powerful,\ninteractive tool that can efficiently fetch and process web content, handle\nvarious input types, and provide informed responses based on the gathered\ninformation.\n\n"
    },
    {
      "title": "Crawl Request Parameters",
      "url": "https://crawl4ai.com/mkdocs/full_details/crawl_request_parameters/",
      "content": "# Crawl Request Parameters for AsyncWebCrawler\n\nThe `arun` method in Crawl4AI's `AsyncWebCrawler` is designed to be highly\nconfigurable, allowing you to customize the crawling and extraction process to\nsuit your needs. Below are the parameters you can use with the `arun` method,\nalong with their descriptions, possible values, and examples.\n\n## Parameters\n\n### url (str)\n\n**Description:** The URL of the webpage to crawl. **Required:** Yes\n**Example:**\n\n    \n    \n    url = \"https://www.nbcnews.com/business\"\n    \n\n### word_count_threshold (int)\n\n**Description:** The minimum number of words a block must contain to be\nconsidered meaningful. The default value is defined by `MIN_WORD_THRESHOLD`.\n**Required:** No **Default Value:** `MIN_WORD_THRESHOLD` **Example:**\n\n    \n    \n    word_count_threshold = 10\n    \n\n### extraction_strategy (ExtractionStrategy)\n\n**Description:** The strategy to use for extracting content from the HTML. It\nmust be an instance of `ExtractionStrategy`. If not provided, the default is\n`NoExtractionStrategy`. **Required:** No **Default Value:**\n`NoExtractionStrategy()` **Example:**\n\n    \n    \n    extraction_strategy = CosineStrategy(semantic_filter=\"finance\")\n    \n\n### chunking_strategy (ChunkingStrategy)\n\n**Description:** The strategy to use for chunking the text before processing.\nIt must be an instance of `ChunkingStrategy`. The default value is\n`RegexChunking()`. **Required:** No **Default Value:** `RegexChunking()`\n**Example:**\n\n    \n    \n    chunking_strategy = NlpSentenceChunking()\n    \n\n### bypass_cache (bool)\n\n**Description:** Whether to force a fresh crawl even if the URL has been\npreviously crawled. The default value is `False`. **Required:** No **Default\nValue:** `False` **Example:**\n\n    \n    \n    bypass_cache = True\n    \n\n### css_selector (str)\n\n**Description:** The CSS selector to target specific parts of the HTML for\nextraction. If not provided, the entire HTML will be processed. **Required:**\nNo **Default Value:** `None` **Example:**\n\n    \n    \n    css_selector = \"div.article-content\"\n    \n\n### screenshot (bool)\n\n**Description:** Whether to take screenshots of the page. The default value is\n`False`. **Required:** No **Default Value:** `False` **Example:**\n\n    \n    \n    screenshot = True\n    \n\n### user_agent (str)\n\n**Description:** The user agent to use for the HTTP requests. If not provided,\na default user agent will be used. **Required:** No **Default Value:** `None`\n**Example:**\n\n    \n    \n    user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n    \n\n### verbose (bool)\n\n**Description:** Whether to enable verbose logging. The default value is\n`True`. **Required:** No **Default Value:** `True` **Example:**\n\n    \n    \n    verbose = True\n    \n\n### **kwargs\n\nAdditional keyword arguments that can be passed to customize the crawling\nprocess further. Some notable options include:\n\n  * **only_text (bool):** Whether to extract only text content, excluding HTML tags. Default is `False`.\n  * **session_id (str):** A unique identifier for the crawling session. This is useful for maintaining state across multiple requests.\n  * **js_code (str or list):** JavaScript code to be executed on the page before extraction.\n  * **wait_for (str):** A CSS selector or JavaScript function to wait for before considering the page load complete.\n\n**Example:**\n\n    \n    \n    result = await crawler.arun(\n        url=\"https://www.nbcnews.com/business\",\n        css_selector=\"p\",\n        only_text=True,\n        session_id=\"unique_session_123\",\n        js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n        wait_for=\"article.main-article\"\n    )\n    \n\n## Example Usage\n\nHere's an example of how to use the `arun` method with various parameters:\n\n    \n    \n    import asyncio\n    from crawl4ai import AsyncWebCrawler\n    from crawl4ai.extraction_strategy import CosineStrategy\n    from crawl4ai.chunking_strategy import NlpSentenceChunking\n    \n    async def main():\n        # Create the AsyncWebCrawler instance \n        async with AsyncWebCrawler(verbose=True) as crawler:\n            # Run the crawler with custom parameters\n            result = await crawler.arun(\n                url=\"https://www.nbcnews.com/business\",\n                word_count_threshold=10,\n                extraction_strategy=CosineStrategy(semantic_filter=\"finance\"),\n                chunking_strategy=NlpSentenceChunking(),\n                bypass_cache=True,\n                css_selector=\"div.article-content\",\n                screenshot=True,\n                user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\",\n                verbose=True,\n                only_text=True,\n                session_id=\"business_news_session\",\n                js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n                wait_for=\"footer\"\n            )\n    \n            print(result)\n    \n    # Run the async function\n    asyncio.run(main())\n    \n\nThis example demonstrates how to configure various parameters to customize the\ncrawling and extraction process using the asynchronous version of Crawl4AI.\n\n## Additional Asynchronous Methods\n\nThe `AsyncWebCrawler` class also provides other useful asynchronous methods:\n\n### arun_many\n\n**Description:** Crawl multiple URLs concurrently. **Example:**\n\n    \n    \n    urls = [\"https://example1.com\", \"https://example2.com\", \"https://example3.com\"]\n    results = await crawler.arun_many(urls, word_count_threshold=10, bypass_cache=True)\n    \n\n### aclear_cache\n\n**Description:** Clear the crawler's cache. **Example:**\n\n    \n    \n    await crawler.aclear_cache()\n    \n\n### aflush_cache\n\n**Description:** Completely flush the crawler's cache. **Example:**\n\n    \n    \n    await crawler.aflush_cache()\n    \n\n### aget_cache_size\n\n**Description:** Get the current size of the cache. **Example:**\n\n    \n    \n    cache_size = await crawler.aget_cache_size()\n    print(f\"Current cache size: {cache_size}\")\n    \n\nThese asynchronous methods allow for efficient and flexible use of the\nAsyncWebCrawler in various scenarios.\n\n"
    },
    {
      "title": "Crawl Result Class",
      "url": "https://crawl4ai.com/mkdocs/full_details/crawl_result_class/",
      "content": "# Crawl Result\n\nThe `CrawlResult` class is the heart of Crawl4AI's output, encapsulating all\nthe data extracted from a crawling session. This class contains various fields\nthat store the results of the web crawling and extraction process. Let's break\ndown each field and see what it holds. ðŸŽ‰\n\n## Class Definition\n\n    \n    \n    from pydantic import BaseModel\n    from typing import Dict, List, Optional\n    \n    class CrawlResult(BaseModel):\n        url: str\n        html: str\n        success: bool\n        cleaned_html: Optional[str] = None\n        media: Dict[str, List[Dict]] = {}\n        links: Dict[str, List[Dict]] = {}\n        screenshot: Optional[str] = None\n        markdown: Optional[str] = None\n        extracted_content: Optional[str] = None\n        metadata: Optional[dict] = None\n        error_message: Optional[str] = None\n        session_id: Optional[str] = None\n        responser_headers: Optional[dict] = None\n        status_code: Optional[int] = None\n    \n\n## Fields Explanation\n\n### `url: str`\n\nThe URL that was crawled. This field simply stores the URL of the web page\nthat was processed.\n\n### `html: str`\n\nThe raw HTML content of the web page. This is the unprocessed HTML source as\nretrieved by the crawler.\n\n### `success: bool`\n\nA flag indicating whether the crawling and extraction were successful. If any\nerror occurs during the process, this will be `False`.\n\n### `cleaned_html: Optional[str]`\n\nThe cleaned HTML content of the web page. This field holds the HTML after\nremoving unwanted tags like `<script>`, `<style>`, and others that do not\ncontribute to the useful content.\n\n### `media: Dict[str, List[Dict]]`\n\nA dictionary containing lists of extracted media elements from the web page.\nThe media elements are categorized into images, videos, and audios. Here's how\nthey are structured:\n\n  * **Images** : Each image is represented as a dictionary with `src` (source URL) and `alt` (alternate text).\n  * **Videos** : Each video is represented similarly with `src` and `alt`.\n  * **Audios** : Each audio is represented with `src` and `alt`.\n\n    \n    \n    media = {\n        'images': [\n            {'src': 'image_url1', 'alt': 'description1', \"type\": \"image\"},\n            {'src': 'image_url2', 'alt': 'description2', \"type\": \"image\"}\n        ],\n        'videos': [\n            {'src': 'video_url1', 'alt': 'description1', \"type\": \"video\"}\n        ],\n        'audios': [\n            {'src': 'audio_url1', 'alt': 'description1', \"type\": \"audio\"}\n        ]\n    }\n    \n\n### `links: Dict[str, List[Dict]]`\n\nA dictionary containing lists of internal and external links extracted from\nthe web page. Each link is represented as a dictionary with `href` (URL) and\n`text` (link text).\n\n  * **Internal Links** : Links pointing to the same domain.\n  * **External Links** : Links pointing to different domains.\n\n    \n    \n    links = {\n        'internal': [\n            {'href': 'internal_link1', 'text': 'link_text1'},\n            {'href': 'internal_link2', 'text': 'link_text2'}\n        ],\n        'external': [\n            {'href': 'external_link1', 'text': 'link_text1'}\n        ]\n    }\n    \n\n### `screenshot: Optional[str]`\n\nA base64-encoded screenshot of the web page. This field stores the screenshot\ndata if the crawling was configured to take a screenshot.\n\n### `markdown: Optional[str]`\n\nThe content of the web page converted to Markdown format. This is useful for\ngenerating clean, readable text that retains the structure of the original\nHTML.\n\n### `extracted_content: Optional[str]`\n\nThe content extracted based on the specified extraction strategy. This field\nholds the meaningful content blocks extracted from the web page, ready for\nyour AI and data processing needs.\n\n### `metadata: Optional[dict]`\n\nA dictionary containing metadata extracted from the web page, such as title,\ndescription, keywords, and other meta tags.\n\n### `error_message: Optional[str]`\n\nIf an error occurs during crawling, this field will contain the error message,\nhelping you debug and understand what went wrong. ðŸš¨\n\n### `session_id: Optional[str]`\n\nA unique identifier for the crawling session. This can be useful for tracking\nand managing multiple crawling sessions.\n\n### `responser_headers: Optional[dict]`\n\nA dictionary containing the response headers from the web server. This can\nprovide additional information about the server and the response.\n\n### `status_code: Optional[int]`\n\nThe HTTP status code of the response. This indicates the success or failure of\nthe HTTP request (e.g., 200 for success, 404 for not found, etc.).\n\n"
    },
    {
      "title": "Session Based Crawling",
      "url": "https://crawl4ai.com/mkdocs/full_details/session_based_crawling/",
      "content": "# Session-Based Crawling for Dynamic Content\n\nIn modern web applications, content is often loaded dynamically without\nchanging the URL. Examples include \"Load More\" buttons, infinite scrolling, or\npaginated content that updates via JavaScript. To effectively crawl such\nwebsites, Crawl4AI provides powerful session-based crawling capabilities.\n\nThis guide will explore advanced techniques for crawling dynamic content using\nCrawl4AI's session management features.\n\n## Understanding Session-Based Crawling\n\nSession-based crawling allows you to maintain a persistent browser session\nacross multiple requests. This is crucial when:\n\n  1. The content changes dynamically without URL changes\n  2. You need to interact with the page (e.g., clicking buttons) between requests\n  3. The site requires authentication or maintains state across pages\n\nCrawl4AI's `AsyncWebCrawler` class supports session-based crawling through the\n`session_id` parameter and related methods.\n\n## Basic Concepts\n\nBefore diving into examples, let's review some key concepts:\n\n  * **Session ID** : A unique identifier for a browsing session. Use the same `session_id` across multiple `arun` calls to maintain state.\n  * **JavaScript Execution** : Use the `js_code` parameter to execute JavaScript on the page, such as clicking a \"Load More\" button.\n  * **CSS Selectors** : Use these to target specific elements for extraction or interaction.\n  * **Extraction Strategy** : Define how to extract structured data from the page.\n  * **Wait Conditions** : Specify conditions to wait for before considering the page loaded.\n\n## Example 1: Basic Session-Based Crawling\n\nLet's start with a basic example of session-based crawling:\n\n    \n    \n    import asyncio\n    from crawl4ai import AsyncWebCrawler\n    \n    async def basic_session_crawl():\n        async with AsyncWebCrawler(verbose=True) as crawler:\n            session_id = \"my_session\"\n            url = \"https://example.com/dynamic-content\"\n    \n            for page in range(3):\n                result = await crawler.arun(\n                    url=url,\n                    session_id=session_id,\n                    js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                    css_selector=\".content-item\",\n                    bypass_cache=True\n                )\n    \n                print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n    \n            await crawler.crawler_strategy.kill_session(session_id)\n    \n    asyncio.run(basic_session_crawl())\n    \n\nThis example demonstrates: 1\\. Using a consistent `session_id` across multiple\n`arun` calls 2\\. Executing JavaScript to load more content after the first\npage 3\\. Using a CSS selector to extract specific content 4\\. Properly closing\nthe session after crawling\n\n## Advanced Technique 1: Custom Execution Hooks\n\nCrawl4AI allows you to set custom hooks that execute at different stages of\nthe crawling process. This is particularly useful for handling complex loading\nscenarios.\n\nHere's an example that waits for new content to appear before proceeding:\n\n    \n    \n    async def advanced_session_crawl_with_hooks():\n        first_commit = \"\"\n    \n        async def on_execution_started(page):\n            nonlocal first_commit\n            try:\n                while True:\n                    await page.wait_for_selector(\"li.commit-item h4\")\n                    commit = await page.query_selector(\"li.commit-item h4\")\n                    commit = await commit.evaluate(\"(element) => element.textContent\")\n                    commit = commit.strip()\n                    if commit and commit != first_commit:\n                        first_commit = commit\n                        break\n                    await asyncio.sleep(0.5)\n            except Exception as e:\n                print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n    \n        async with AsyncWebCrawler(verbose=True) as crawler:\n            crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n    \n            url = \"https://github.com/example/repo/commits/main\"\n            session_id = \"commit_session\"\n            all_commits = []\n    \n            js_next_page = \"\"\"\n            const button = document.querySelector('a.pagination-next');\n            if (button) button.click();\n            \"\"\"\n    \n            for page in range(3):\n                result = await crawler.arun(\n                    url=url,\n                    session_id=session_id,\n                    css_selector=\"li.commit-item\",\n                    js_code=js_next_page if page > 0 else None,\n                    bypass_cache=True,\n                    js_only=page > 0\n                )\n    \n                commits = result.extracted_content.select(\"li.commit-item\")\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n    \n            await crawler.crawler_strategy.kill_session(session_id)\n            print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n    \n    asyncio.run(advanced_session_crawl_with_hooks())\n    \n\nThis technique uses a custom `on_execution_started` hook to ensure new content\nhas loaded before proceeding to the next step.\n\n## Advanced Technique 2: Integrated JavaScript Execution and Waiting\n\nInstead of using separate hooks, you can integrate the waiting logic directly\ninto your JavaScript execution. This approach can be more concise and easier\nto manage for some scenarios.\n\nHere's an example:\n\n    \n    \n    async def integrated_js_and_wait_crawl():\n        async with AsyncWebCrawler(verbose=True) as crawler:\n            url = \"https://github.com/example/repo/commits/main\"\n            session_id = \"integrated_session\"\n            all_commits = []\n    \n            js_next_page_and_wait = \"\"\"\n            (async () => {\n                const getCurrentCommit = () => {\n                    const commits = document.querySelectorAll('li.commit-item h4');\n                    return commits.length > 0 ? commits[0].textContent.trim() : null;\n                };\n    \n                const initialCommit = getCurrentCommit();\n                const button = document.querySelector('a.pagination-next');\n                if (button) button.click();\n    \n                while (true) {\n                    await new Promise(resolve => setTimeout(resolve, 100));\n                    const newCommit = getCurrentCommit();\n                    if (newCommit && newCommit !== initialCommit) {\n                        break;\n                    }\n                }\n            })();\n            \"\"\"\n    \n            schema = {\n                \"name\": \"Commit Extractor\",\n                \"baseSelector\": \"li.commit-item\",\n                \"fields\": [\n                    {\n                        \"name\": \"title\",\n                        \"selector\": \"h4.commit-title\",\n                        \"type\": \"text\",\n                        \"transform\": \"strip\",\n                    },\n                ],\n            }\n            extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n    \n            for page in range(3):\n                result = await crawler.arun(\n                    url=url,\n                    session_id=session_id,\n                    css_selector=\"li.commit-item\",\n                    extraction_strategy=extraction_strategy,\n                    js_code=js_next_page_and_wait if page > 0 else None,\n                    js_only=page > 0,\n                    bypass_cache=True\n                )\n    \n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n    \n            await crawler.crawler_strategy.kill_session(session_id)\n            print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n    \n    asyncio.run(integrated_js_and_wait_crawl())\n    \n\nThis approach combines the JavaScript for clicking the \"next\" button and\nwaiting for new content to load into a single script.\n\n## Advanced Technique 3: Using the `wait_for` Parameter\n\nCrawl4AI provides a `wait_for` parameter that allows you to specify a\ncondition to wait for before considering the page fully loaded. This can be\nparticularly useful for dynamic content.\n\nHere's an example:\n\n    \n    \n    async def wait_for_parameter_crawl():\n        async with AsyncWebCrawler(verbose=True) as crawler:\n            url = \"https://github.com/example/repo/commits/main\"\n            session_id = \"wait_for_session\"\n            all_commits = []\n    \n            js_next_page = \"\"\"\n            const commits = document.querySelectorAll('li.commit-item h4');\n            if (commits.length > 0) {\n                window.lastCommit = commits[0].textContent.trim();\n            }\n            const button = document.querySelector('a.pagination-next');\n            if (button) button.click();\n            \"\"\"\n    \n            wait_for = \"\"\"() => {\n                const commits = document.querySelectorAll('li.commit-item h4');\n                if (commits.length === 0) return false;\n                const firstCommit = commits[0].textContent.trim();\n                return firstCommit !== window.lastCommit;\n            }\"\"\"\n    \n            schema = {\n                \"name\": \"Commit Extractor\",\n                \"baseSelector\": \"li.commit-item\",\n                \"fields\": [\n                    {\n                        \"name\": \"title\",\n                        \"selector\": \"h4.commit-title\",\n                        \"type\": \"text\",\n                        \"transform\": \"strip\",\n                    },\n                ],\n            }\n            extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n    \n            for page in range(3):\n                result = await crawler.arun(\n                    url=url,\n                    session_id=session_id,\n                    css_selector=\"li.commit-item\",\n                    extraction_strategy=extraction_strategy,\n                    js_code=js_next_page if page > 0 else None,\n                    wait_for=wait_for if page > 0 else None,\n                    js_only=page > 0,\n                    bypass_cache=True\n                )\n    \n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n    \n            await crawler.crawler_strategy.kill_session(session_id)\n            print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n    \n    asyncio.run(wait_for_parameter_crawl())\n    \n\nThis technique separates the JavaScript execution (clicking the \"next\" button)\nfrom the waiting condition, providing more flexibility and clarity in some\nscenarios.\n\n## Best Practices for Session-Based Crawling\n\n  1. **Use Unique Session IDs** : Ensure each crawling session has a unique `session_id` to prevent conflicts.\n  2. **Close Sessions** : Always close sessions using `kill_session` when you're done to free up resources.\n  3. **Handle Errors** : Implement proper error handling to deal with unexpected situations during crawling.\n  4. **Respect Website Terms** : Ensure your crawling adheres to the website's terms of service and robots.txt file.\n  5. **Implement Delays** : Add appropriate delays between requests to avoid overwhelming the target server.\n  6. **Use Extraction Strategies** : Leverage `JsonCssExtractionStrategy` or other extraction strategies for structured data extraction.\n  7. **Optimize JavaScript** : Keep your JavaScript execution concise and efficient to improve crawling speed.\n  8. **Monitor Performance** : Keep an eye on memory usage and crawling speed, especially for long-running sessions.\n\n## Conclusion\n\nSession-based crawling with Crawl4AI provides powerful capabilities for\nhandling dynamic content and complex web applications. By leveraging session\nmanagement, JavaScript execution, and waiting strategies, you can effectively\ncrawl and extract data from a wide range of modern websites.\n\nRemember to use these techniques responsibly and in compliance with website\npolicies and ethical web scraping practices.\n\nFor more advanced usage and API details, refer to the Crawl4AI API\ndocumentation.\n\n"
    },
    {
      "title": "Advanced Features",
      "url": "https://crawl4ai.com/mkdocs/full_details/advanced_features/",
      "content": "# Advanced Features\n\nCrawl4AI offers a range of advanced features that allow you to fine-tune your\nweb crawling and data extraction process. This section will cover some of\nthese advanced features, including taking screenshots, extracting media and\nlinks, customizing the user agent, using custom hooks, and leveraging CSS\nselectors.\n\n## Taking Screenshots ðŸ“¸\n\nOne of the cool features of Crawl4AI is the ability to take screenshots of the\nweb pages you're crawling. This can be particularly useful for visual\nverification or for capturing the state of dynamic content.\n\nHere's how you can take a screenshot:\n\n    \n    \n    from crawl4ai import WebCrawler\n    import base64\n    \n    # Create the WebCrawler instance\n    crawler = WebCrawler()\n    crawler.warmup()\n    \n    # Run the crawler with the screenshot parameter\n    result = crawler.run(url=\"https://www.nbcnews.com/business\", screenshot=True)\n    \n    # Save the screenshot to a file\n    with open(\"screenshot.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n    \n    print(\"Screenshot saved to 'screenshot.png'!\")\n    \n\nIn this example, we create a `WebCrawler` instance, warm it up, and then run\nit with the `screenshot` parameter set to `True`. The screenshot is saved as a\nbase64 encoded string in the result, which we then decode and save as a PNG\nfile.\n\n## Extracting Media and Links ðŸŽ¨ðŸ”—\n\nCrawl4AI can extract all media tags (images, audio, and video) and links (both\ninternal and external) from a web page. This feature is useful for collecting\nmultimedia content or analyzing link structures.\n\nHere's an example:\n\n    \n    \n    from crawl4ai import WebCrawler\n    \n    # Create the WebCrawler instance\n    crawler = WebCrawler()\n    crawler.warmup()\n    \n    # Run the crawler\n    result = crawler.run(url=\"https://www.nbcnews.com/business\")\n    \n    print(\"Extracted media:\", result.media)\n    print(\"Extracted links:\", result.links)\n    \n\nIn this example, the `result` object contains dictionaries for media and\nlinks, which you can access and use as needed.\n\n## Customizing the User Agent ðŸ•µï¸â€â™‚ï¸\n\nCrawl4AI allows you to set a custom user agent for your HTTP requests. This\ncan help you avoid detection by web servers or simulate different browsing\nenvironments.\n\nHere's how to set a custom user agent:\n\n    \n    \n    from crawl4ai import WebCrawler\n    \n    # Create the WebCrawler instance\n    crawler = WebCrawler()\n    crawler.warmup()\n    \n    # Run the crawler with a custom user agent\n    result = crawler.run(url=\"https://www.nbcnews.com/business\", user_agent=\"Mozilla/5.0 (compatible; MyCrawler/1.0)\")\n    \n    print(\"Crawl result:\", result)\n    \n\nIn this example, we specify a custom user agent string when running the\ncrawler.\n\n## Using Custom Hooks ðŸª\n\nHooks are a powerful feature in Crawl4AI that allow you to customize the\ncrawling process at various stages. You can define hooks for actions such as\ndriver initialization, before and after URL fetching, and before returning the\nHTML.\n\nHere's an example of using hooks:\n\n    \n    \n    from crawl4ai import WebCrawler\n    from selenium.webdriver.common.by import By\n    from selenium.webdriver.support.ui import WebDriverWait\n    from selenium.webdriver.support import expected_conditions as EC\n    \n    # Define the hooks\n    def on_driver_created(driver):\n        driver.maximize_window()\n        driver.get('https://example.com/login')\n        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.NAME, 'username'))).send_keys('testuser')\n        driver.find_element(By.NAME, 'password').send_keys('password123')\n        driver.find_element(By.NAME, 'login').click()\n        return driver\n    \n    def before_get_url(driver):\n        driver.execute_cdp_cmd('Network.setExtraHTTPHeaders', {'headers': {'X-Test-Header': 'test'}})\n        return driver\n    \n    # Create the WebCrawler instance\n    crawler = WebCrawler()\n    crawler.warmup()\n    \n    # Set the hooks\n    crawler.set_hook('on_driver_created', on_driver_created)\n    crawler.set_hook('before_get_url', before_get_url)\n    \n    # Run the crawler\n    result = crawler.run(url=\"https://example.com\")\n    \n    print(\"Crawl result:\", result)\n    \n\nIn this example, we define hooks to handle driver initialization and custom\nheaders before fetching the URL.\n\n## Using CSS Selectors ðŸŽ¯\n\nCSS selectors allow you to target specific elements on a web page for\nextraction. This can be useful for scraping structured content, such as\narticles or product details.\n\nHere's an example of using a CSS selector:\n\n    \n    \n    from crawl4ai import WebCrawler\n    \n    # Create the WebCrawler instance\n    crawler = WebCrawler()\n    crawler.warmup()\n    \n    # Run the crawler with a CSS selector to extract only H2 tags\n    result = crawler.run(url=\"https://www.nbcnews.com/business\", css_selector=\"h2\")\n    \n    print(\"Extracted H2 tags:\", result.extracted_content)\n    \n\nIn this example, we use the `css_selector` parameter to extract only the H2\ntags from the web page.\n\n* * *\n\nWith these advanced features, you can leverage Crawl4AI to perform\nsophisticated web crawling and data extraction tasks. Whether you need to take\nscreenshots, extract specific elements, customize the crawling process, or set\ncustom headers, Crawl4AI provides the flexibility and power to meet your\nneeds. Happy crawling! ðŸ•·ï¸ðŸš€\n\n"
    },
    {
      "title": "Advanced JsonCssExtraction",
      "url": "https://crawl4ai.com/mkdocs/full_details/advanced_jsoncss_extraction/",
      "content": "# Advanced Usage of JsonCssExtractionStrategy\n\nWhile the basic usage of JsonCssExtractionStrategy is powerful for simple\nstructures, its true potential shines when dealing with complex, nested HTML\nstructures. This section will explore advanced usage scenarios, demonstrating\nhow to extract nested objects, lists, and nested lists.\n\n## Hypothetical Website Example\n\nLet's consider a hypothetical e-commerce website that displays product\ncategories, each containing multiple products. Each product has details,\nreviews, and related items. This complex structure will allow us to\ndemonstrate various advanced features of JsonCssExtractionStrategy.\n\nAssume the HTML structure looks something like this:\n\n    \n    \n    <div class=\"category\">\n      <h2 class=\"category-name\">Electronics</h2>\n      <div class=\"product\">\n        <h3 class=\"product-name\">Smartphone X</h3>\n        <p class=\"product-price\">$999</p>\n        <div class=\"product-details\">\n          <span class=\"brand\">TechCorp</span>\n          <span class=\"model\">X-2000</span>\n        </div>\n        <ul class=\"product-features\">\n          <li>5G capable</li>\n          <li>6.5\" OLED screen</li>\n          <li>128GB storage</li>\n        </ul>\n        <div class=\"product-reviews\">\n          <div class=\"review\">\n            <span class=\"reviewer\">John D.</span>\n            <span class=\"rating\">4.5</span>\n            <p class=\"review-text\">Great phone, love the camera!</p>\n          </div>\n          <div class=\"review\">\n            <span class=\"reviewer\">Jane S.</span>\n            <span class=\"rating\">5</span>\n            <p class=\"review-text\">Best smartphone I've ever owned.</p>\n          </div>\n        </div>\n        <ul class=\"related-products\">\n          <li>\n            <span class=\"related-name\">Phone Case</span>\n            <span class=\"related-price\">$29.99</span>\n          </li>\n          <li>\n            <span class=\"related-name\">Screen Protector</span>\n            <span class=\"related-price\">$9.99</span>\n          </li>\n        </ul>\n      </div>\n      <!-- More products... -->\n    </div>\n    \n\nNow, let's create a schema to extract this complex structure:\n\n    \n    \n    schema = {\n        \"name\": \"E-commerce Product Catalog\",\n        \"baseSelector\": \"div.category\",\n        \"fields\": [\n            {\n                \"name\": \"category_name\",\n                \"selector\": \"h2.category-name\",\n                \"type\": \"text\"\n            },\n            {\n                \"name\": \"products\",\n                \"selector\": \"div.product\",\n                \"type\": \"nested_list\",\n                \"fields\": [\n                    {\n                        \"name\": \"name\",\n                        \"selector\": \"h3.product-name\",\n                        \"type\": \"text\"\n                    },\n                    {\n                        \"name\": \"price\",\n                        \"selector\": \"p.product-price\",\n                        \"type\": \"text\"\n                    },\n                    {\n                        \"name\": \"details\",\n                        \"selector\": \"div.product-details\",\n                        \"type\": \"nested\",\n                        \"fields\": [\n                            {\n                                \"name\": \"brand\",\n                                \"selector\": \"span.brand\",\n                                \"type\": \"text\"\n                            },\n                            {\n                                \"name\": \"model\",\n                                \"selector\": \"span.model\",\n                                \"type\": \"text\"\n                            }\n                        ]\n                    },\n                    {\n                        \"name\": \"features\",\n                        \"selector\": \"ul.product-features li\",\n                        \"type\": \"list\",\n                        \"fields\": [\n                            {\n                                \"name\": \"feature\",\n                                \"type\": \"text\"\n                            }\n                        ]\n                    },\n                    {\n                        \"name\": \"reviews\",\n                        \"selector\": \"div.review\",\n                        \"type\": \"nested_list\",\n                        \"fields\": [\n                            {\n                                \"name\": \"reviewer\",\n                                \"selector\": \"span.reviewer\",\n                                \"type\": \"text\"\n                            },\n                            {\n                                \"name\": \"rating\",\n                                \"selector\": \"span.rating\",\n                                \"type\": \"text\"\n                            },\n                            {\n                                \"name\": \"comment\",\n                                \"selector\": \"p.review-text\",\n                                \"type\": \"text\"\n                            }\n                        ]\n                    },\n                    {\n                        \"name\": \"related_products\",\n                        \"selector\": \"ul.related-products li\",\n                        \"type\": \"list\",\n                        \"fields\": [\n                            {\n                                \"name\": \"name\",\n                                \"selector\": \"span.related-name\",\n                                \"type\": \"text\"\n                            },\n                            {\n                                \"name\": \"price\",\n                                \"selector\": \"span.related-price\",\n                                \"type\": \"text\"\n                            }\n                        ]\n                    }\n                ]\n            }\n        ]\n    }\n    \n\nThis schema demonstrates several advanced features:\n\n  1. **Nested Objects** : The `details` field is a nested object within each product.\n  2. **Simple Lists** : The `features` field is a simple list of text items.\n  3. **Nested Lists** : The `products` field is a nested list, where each item is a complex object.\n  4. **Lists of Objects** : The `reviews` and `related_products` fields are lists of objects.\n\nLet's break down the key concepts:\n\n### Nested Objects\n\nTo create a nested object, use `\"type\": \"nested\"` and provide a `fields` array\nfor the nested structure:\n\n    \n    \n    {\n        \"name\": \"details\",\n        \"selector\": \"div.product-details\",\n        \"type\": \"nested\",\n        \"fields\": [\n            {\n                \"name\": \"brand\",\n                \"selector\": \"span.brand\",\n                \"type\": \"text\"\n            },\n            {\n                \"name\": \"model\",\n                \"selector\": \"span.model\",\n                \"type\": \"text\"\n            }\n        ]\n    }\n    \n\n### Simple Lists\n\nFor a simple list of identical items, use `\"type\": \"list\"`:\n\n    \n    \n    {\n        \"name\": \"features\",\n        \"selector\": \"ul.product-features li\",\n        \"type\": \"list\",\n        \"fields\": [\n            {\n                \"name\": \"feature\",\n                \"type\": \"text\"\n            }\n        ]\n    }\n    \n\n### Nested Lists\n\nFor a list of complex objects, use `\"type\": \"nested_list\"`:\n\n    \n    \n    {\n        \"name\": \"products\",\n        \"selector\": \"div.product\",\n        \"type\": \"nested_list\",\n        \"fields\": [\n            // ... fields for each product\n        ]\n    }\n    \n\n### Lists of Objects\n\nSimilar to nested lists, but typically used for simpler objects within the\nlist:\n\n    \n    \n    {\n        \"name\": \"related_products\",\n        \"selector\": \"ul.related-products li\",\n        \"type\": \"list\",\n        \"fields\": [\n            {\n                \"name\": \"name\",\n                \"selector\": \"span.related-name\",\n                \"type\": \"text\"\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"span.related-price\",\n                \"type\": \"text\"\n            }\n        ]\n    }\n    \n\n## Using the Advanced Schema\n\nTo use this advanced schema with AsyncWebCrawler:\n\n    \n    \n    import json\n    import asyncio\n    from crawl4ai import AsyncWebCrawler\n    from crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n    \n    async def extract_complex_product_data():\n        extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n    \n        async with AsyncWebCrawler(verbose=True) as crawler:\n            result = await crawler.arun(\n                url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n                extraction_strategy=extraction_strategy,\n                bypass_cache=True,\n            )\n    \n            assert result.success, \"Failed to crawl the page\"\n    \n            product_data = json.loads(result.extracted_content)\n            print(json.dumps(product_data, indent=2))\n    \n    asyncio.run(extract_complex_product_data())\n    \n\nThis will produce a structured JSON output that captures the complex hierarchy\nof the product catalog, including nested objects, lists, and nested lists.\n\n## Tips for Advanced Usage\n\n  1. **Start Simple** : Begin with a basic schema and gradually add complexity.\n  2. **Test Incrementally** : Test each part of your schema separately before combining them.\n  3. **Use Chrome DevTools** : The Element Inspector is invaluable for identifying the correct selectors.\n  4. **Handle Missing Data** : Use the `default` key in your field definitions to handle cases where data might be missing.\n  5. **Leverage Transforms** : Use the `transform` key to clean or format extracted data (e.g., converting prices to numbers).\n  6. **Consider Performance** : Very complex schemas might slow down extraction. Balance complexity with performance needs.\n\nBy mastering these advanced techniques, you can use JsonCssExtractionStrategy\nto extract highly structured data from even the most complex web pages, making\nit a powerful tool for web scraping and data analysis tasks.\n\n"
    },
    {
      "title": "Chunking Strategies",
      "url": "https://crawl4ai.com/mkdocs/full_details/chunking_strategies/",
      "content": "## Chunking Strategies ðŸ“š\n\nCrawl4AI provides several powerful chunking strategies to divide text into\nmanageable parts for further processing. Each strategy has unique\ncharacteristics and is suitable for different scenarios. Let's explore them\none by one.\n\n### RegexChunking\n\n`RegexChunking` splits text using regular expressions. This is ideal for\ncreating chunks based on specific patterns like paragraphs or sentences.\n\n#### When to Use\n\n  * Great for structured text with consistent delimiters.\n  * Suitable for documents where specific patterns (e.g., double newlines, periods) indicate logical chunks.\n\n#### Parameters\n\n  * `patterns` (list, optional): Regular expressions used to split the text. Default is to split by double newlines (`['\\n\\n']`).\n\n#### Example\n\n    \n    \n    from crawl4ai.chunking_strategy import RegexChunking\n    \n    # Define patterns for splitting text\n    patterns = [r'\\n\\n', r'\\. ']\n    chunker = RegexChunking(patterns=patterns)\n    \n    # Sample text\n    text = \"This is a sample text. It will be split into chunks.\\n\\nThis is another paragraph.\"\n    \n    # Chunk the text\n    chunks = chunker.chunk(text)\n    print(chunks)\n    \n\n### NlpSentenceChunking\n\n`NlpSentenceChunking` uses NLP models to split text into sentences, ensuring\naccurate sentence boundaries.\n\n#### When to Use\n\n  * Ideal for texts where sentence boundaries are crucial.\n  * Useful for creating chunks that preserve grammatical structures.\n\n#### Parameters\n\n  * None.\n\n#### Example\n\n    \n    \n    from crawl4ai.chunking_strategy import NlpSentenceChunking\n    \n    chunker = NlpSentenceChunking()\n    \n    # Sample text\n    text = \"This is a sample text. It will be split into sentences. Here's another sentence.\"\n    \n    # Chunk the text\n    chunks = chunker.chunk(text)\n    print(chunks)\n    \n\n### TopicSegmentationChunking\n\n`TopicSegmentationChunking` employs the TextTiling algorithm to segment text\ninto topic-based chunks. This method identifies thematic boundaries.\n\n#### When to Use\n\n  * Perfect for long documents with distinct topics.\n  * Useful when preserving topic continuity is more important than maintaining text order.\n\n#### Parameters\n\n  * `num_keywords` (int, optional): Number of keywords for each topic segment. Default is `3`.\n\n#### Example\n\n    \n    \n    from crawl4ai.chunking_strategy import TopicSegmentationChunking\n    \n    chunker = TopicSegmentationChunking(num_keywords=3)\n    \n    # Sample text\n    text = \"This document contains several topics. Topic one discusses AI. Topic two covers machine learning.\"\n    \n    # Chunk the text\n    chunks = chunker.chunk(text)\n    print(chunks)\n    \n\n### FixedLengthWordChunking\n\n`FixedLengthWordChunking` splits text into chunks based on a fixed number of\nwords. This ensures each chunk has approximately the same length.\n\n#### When to Use\n\n  * Suitable for processing large texts where uniform chunk size is important.\n  * Useful when the number of words per chunk needs to be controlled.\n\n#### Parameters\n\n  * `chunk_size` (int, optional): Number of words per chunk. Default is `100`.\n\n#### Example\n\n    \n    \n    from crawl4ai.chunking_strategy import FixedLengthWordChunking\n    \n    chunker = FixedLengthWordChunking(chunk_size=10)\n    \n    # Sample text\n    text = \"This is a sample text. It will be split into chunks of fixed length.\"\n    \n    # Chunk the text\n    chunks = chunker.chunk(text)\n    print(chunks)\n    \n\n### SlidingWindowChunking\n\n`SlidingWindowChunking` uses a sliding window approach to create overlapping\nchunks. Each chunk has a fixed length, and the window slides by a specified\nstep size.\n\n#### When to Use\n\n  * Ideal for creating overlapping chunks to preserve context.\n  * Useful for tasks where context from adjacent chunks is needed.\n\n#### Parameters\n\n  * `window_size` (int, optional): Number of words in each chunk. Default is `100`.\n  * `step` (int, optional): Number of words to slide the window. Default is `50`.\n\n#### Example\n\n    \n    \n    from crawl4ai.chunking_strategy import SlidingWindowChunking\n    \n    chunker = SlidingWindowChunking(window_size=10, step=5)\n    \n    # Sample text\n    text = \"This is a sample text. It will be split using a sliding window approach to preserve context.\"\n    \n    # Chunk the text\n    chunks = chunker.chunk(text)\n    print(chunks)\n    \n\nWith these chunking strategies, you can choose the best method to divide your\ntext based on your specific needs. Whether you need precise sentence\nboundaries, topic-based segmentation, or uniform chunk sizes, Crawl4AI has you\ncovered. Happy chunking! ðŸ“âœ¨\n\n"
    },
    {
      "title": "Extraction Strategies",
      "url": "https://crawl4ai.com/mkdocs/full_details/extraction_strategies/",
      "content": "## Extraction Strategies ðŸ§ \n\nCrawl4AI offers powerful extraction strategies to derive meaningful\ninformation from web content. Let's dive into three of the most important\nstrategies: `CosineStrategy`, `LLMExtractionStrategy`, and the new\n`JsonCssExtractionStrategy`.\n\n### LLMExtractionStrategy\n\n`LLMExtractionStrategy` leverages a Language Model (LLM) to extract meaningful\ncontent from HTML. This strategy uses an external provider for LLM completions\nto perform extraction based on instructions.\n\n#### When to Use\n\n  * Suitable for complex extraction tasks requiring nuanced understanding.\n  * Ideal for scenarios where detailed instructions can guide the extraction process.\n  * Perfect for extracting specific types of information or content with precise guidelines.\n\n#### Parameters\n\n  * `provider` (str, optional): Provider for language model completions (e.g., openai/gpt-4). Default is `DEFAULT_PROVIDER`.\n  * `api_token` (str, optional): API token for the provider. If not provided, it will try to load from the environment variable `OPENAI_API_KEY`.\n  * `instruction` (str, optional): Instructions to guide the LLM on how to perform the extraction. Default is `None`.\n\n#### Example Without Instructions\n\n    \n    \n    import asyncio\n    import os\n    from crawl4ai import AsyncWebCrawler\n    from crawl4ai.extraction_strategy import LLMExtractionStrategy\n    \n    async def main():\n        async with AsyncWebCrawler(verbose=True) as crawler:\n            # Define extraction strategy without instructions\n            strategy = LLMExtractionStrategy(\n                provider='openai',\n                api_token=os.getenv('OPENAI_API_KEY')\n            )\n    \n            # Sample URL\n            url = \"https://www.nbcnews.com/business\"\n    \n            # Run the crawler with the extraction strategy\n            result = await crawler.arun(url=url, extraction_strategy=strategy)\n            print(result.extracted_content)\n    \n    asyncio.run(main())\n    \n\n#### Example With Instructions\n\n    \n    \n    import asyncio\n    import os\n    from crawl4ai import AsyncWebCrawler\n    from crawl4ai.extraction_strategy import LLMExtractionStrategy\n    \n    async def main():\n        async with AsyncWebCrawler(verbose=True) as crawler:\n            # Define extraction strategy with instructions\n            strategy = LLMExtractionStrategy(\n                provider='openai',\n                api_token=os.getenv('OPENAI_API_KEY'),\n                instruction=\"Extract only financial news and summarize key points.\"\n            )\n    \n            # Sample URL\n            url = \"https://www.nbcnews.com/business\"\n    \n            # Run the crawler with the extraction strategy\n            result = await crawler.arun(url=url, extraction_strategy=strategy)\n            print(result.extracted_content)\n    \n    asyncio.run(main())\n    \n\n### JsonCssExtractionStrategy\n\n`JsonCssExtractionStrategy` is a powerful tool for extracting structured data\nfrom HTML using CSS selectors. It allows you to define a schema that maps CSS\nselectors to specific fields, enabling precise and efficient data extraction.\n\n#### When to Use\n\n  * Ideal for extracting structured data from websites with consistent HTML structures.\n  * Perfect for scenarios where you need to extract specific elements or attributes from a webpage.\n  * Suitable for creating datasets from web pages with tabular or list-based information.\n\n#### Parameters\n\n  * `schema` (Dict[str, Any]): A dictionary defining the extraction schema, including base selector and field definitions.\n\n#### Example\n\n    \n    \n    import asyncio\n    import json\n    from crawl4ai import AsyncWebCrawler\n    from crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n    \n    async def main():\n        async with AsyncWebCrawler(verbose=True) as crawler:\n            # Define the extraction schema\n            schema = {\n                \"name\": \"News Articles\",\n                \"baseSelector\": \"article.tease-card\",\n                \"fields\": [\n                    {\n                        \"name\": \"title\",\n                        \"selector\": \"h2\",\n                        \"type\": \"text\",\n                    },\n                    {\n                        \"name\": \"summary\",\n                        \"selector\": \"div.tease-card__info\",\n                        \"type\": \"text\",\n                    },\n                    {\n                        \"name\": \"link\",\n                        \"selector\": \"a\",\n                        \"type\": \"attribute\",\n                        \"attribute\": \"href\"\n                    }\n                ],\n            }\n    \n            # Create the extraction strategy\n            strategy = JsonCssExtractionStrategy(schema, verbose=True)\n    \n            # Sample URL\n            url = \"https://www.nbcnews.com/business\"\n    \n            # Run the crawler with the extraction strategy\n            result = await crawler.arun(url=url, extraction_strategy=strategy)\n    \n            # Parse and print the extracted content\n            extracted_data = json.loads(result.extracted_content)\n            print(json.dumps(extracted_data, indent=2))\n    \n    asyncio.run(main())\n    \n\n#### Use Cases for JsonCssExtractionStrategy\n\n  * Extracting product information from e-commerce websites.\n  * Gathering news articles and their metadata from news portals.\n  * Collecting user reviews and ratings from review websites.\n  * Extracting job listings from job boards.\n\nBy choosing the right extraction strategy, you can effectively extract the\nmost relevant and useful information from web content. Whether you need fast,\naccurate semantic segmentation with `CosineStrategy`, nuanced, instruction-\nbased extraction with `LLMExtractionStrategy`, or precise structured data\nextraction with `JsonCssExtractionStrategy`, Crawl4AI has you covered. Happy\nextracting! ðŸ•µï¸â€â™‚ï¸âœ¨\n\nFor more details on schema definitions and advanced extraction strategies,\ncheck out the[Advanced JsonCssExtraction](../advanced_jsoncss_extraction/).\n\n### CosineStrategy\n\n`CosineStrategy` uses hierarchical clustering based on cosine similarity to\ngroup text chunks into meaningful clusters. This method converts each chunk\ninto its embedding and then clusters them to form semantical chunks.\n\n#### When to Use\n\n  * Ideal for fast, accurate semantic segmentation of text.\n  * Perfect for scenarios where LLMs might be overkill or too slow.\n  * Suitable for narrowing down content based on specific queries or keywords.\n\n#### Parameters\n\n  * `semantic_filter` (str, optional): Keywords for filtering relevant documents before clustering. Documents are filtered based on their cosine similarity to the keyword filter embedding. Default is `None`.\n  * `word_count_threshold` (int, optional): Minimum number of words per cluster. Default is `20`.\n  * `max_dist` (float, optional): Maximum cophenetic distance on the dendrogram to form clusters. Default is `0.2`.\n  * `linkage_method` (str, optional): Linkage method for hierarchical clustering. Default is `'ward'`.\n  * `top_k` (int, optional): Number of top categories to extract. Default is `3`.\n  * `model_name` (str, optional): Model name for embedding generation. Default is `'BAAI/bge-small-en-v1.5'`.\n\n#### Example\n\n    \n    \n    import asyncio\n    from crawl4ai import AsyncWebCrawler\n    from crawl4ai.extraction_strategy import CosineStrategy\n    \n    async def main():\n        async with AsyncWebCrawler(verbose=True) as crawler:\n            # Define extraction strategy\n            strategy = CosineStrategy(\n                semantic_filter=\"finance economy stock market\",\n                word_count_threshold=10,\n                max_dist=0.2,\n                linkage_method='ward',\n                top_k=3,\n                model_name='BAAI/bge-small-en-v1.5'\n            )\n    \n            # Sample URL\n            url = \"https://www.nbcnews.com/business\"\n    \n            # Run the crawler with the extraction strategy\n            result = await crawler.arun(url=url, extraction_strategy=strategy)\n            print(result.extracted_content)\n    \n    asyncio.run(main())\n    \n\n"
    },
    {
      "title": "Change Log",
      "url": "https://crawl4ai.com/mkdocs/changelog/",
      "content": "# Changelog\n\n## [v0.2.77] - 2024-08-04\n\nSignificant improvements in text processing and performance:\n\n  * ðŸš€ **Dependency reduction** : Removed dependency on spaCy model for text chunk labeling in cosine extraction strategy.\n  * ðŸ¤– **Transformer upgrade** : Implemented text sequence classification using a transformer model for labeling text chunks.\n  * âš¡ **Performance enhancement** : Improved model loading speed due to removal of spaCy dependency.\n  * ðŸ”§ **Future-proofing** : Laid groundwork for potential complete removal of spaCy dependency in future versions.\n\nThese changes address issue #68 and provide a foundation for faster, more\nefficient text processing in Crawl4AI.\n\n## [v0.2.76] - 2024-08-02\n\nMajor improvements in functionality, performance, and cross-platform\ncompatibility! ðŸš€\n\n  * ðŸ³ **Docker enhancements** : Significantly improved Dockerfile for easy installation on Linux, Mac, and Windows.\n  * ðŸŒ **Official Docker Hub image** : Launched our first official image on Docker Hub for streamlined deployment.\n  * ðŸ”§ **Selenium upgrade** : Removed dependency on ChromeDriver, now using Selenium's built-in capabilities for better compatibility.\n  * ðŸ–¼ï¸ **Image description** : Implemented ability to generate textual descriptions for extracted images from web pages.\n  * âš¡ **Performance boost** : Various improvements to enhance overall speed and performance.\n\nA big shoutout to our amazing community contributors: \\-\n[@aravindkarnam](https://github.com/aravindkarnam) for developing the textual\ndescription extraction feature. \\-\n[@FractalMind](https://github.com/FractalMind) for creating the first official\nDocker Hub image and fixing Dockerfile errors. \\-\n[@ketonkss4](https://github.com/ketonkss4) for identifying Selenium's new\ncapabilities, helping us reduce dependencies.\n\nYour contributions are driving Crawl4AI forward! ðŸ™Œ\n\n## [v0.2.75] - 2024-07-19\n\nMinor improvements for a more maintainable codebase:\n\n  * ðŸ”„ Fixed typos in `chunking_strategy.py` and `crawler_strategy.py` to improve code readability\n  * ðŸ”„ Removed `.test_pads/` directory from `.gitignore` to keep our repository clean and organized\n\nThese changes may seem small, but they contribute to a more stable and\nsustainable codebase. By fixing typos and updating our `.gitignore` settings,\nwe're ensuring that our code is easier to maintain and scale in the long run.\n\n## v0.2.74 - 2024-07-08\n\nA slew of exciting updates to improve the crawler's stability and robustness!\nðŸŽ‰\n\n  * ðŸ’» **UTF encoding fix** : Resolved the Windows \\\"charmap\\\" error by adding UTF encoding.\n  * ðŸ›¡ï¸ **Error handling** : Implemented MaxRetryError exception handling in LocalSeleniumCrawlerStrategy.\n  * ðŸ§¹ **Input sanitization** : Improved input sanitization and handled encoding issues in LLMExtractionStrategy.\n  * ðŸš® **Database cleanup** : Removed existing database file and initialized a new one.\n\n## [v0.2.73] - 2024-07-03\n\nðŸ’¡ In this release, we've bumped the version to v0.2.73 and refreshed our\ndocumentation to ensure you have the best experience with our project.\n\n  * Supporting website need \"with-head\" mode to crawl the website with head.\n  * Fixing the installation issues for setup.py and dockerfile.\n  * Resolve multiple issues.\n\n## [v0.2.72] - 2024-06-30\n\nThis release brings exciting updates and improvements to our project! ðŸŽ‰\n\n  * ðŸ“š **Documentation Updates** : Our documentation has been revamped to reflect the latest changes and additions.\n  * ðŸš€ **New Modes in setup.py** : We've added support for three new modes in setup.py: default, torch, and transformers. This enhances the project's flexibility and usability.\n  * ðŸ³ **Docker File Updates** : The Docker file has been updated to ensure seamless compatibility with the new modes and improvements.\n  * ðŸ•·ï¸ **Temporary Solution for Headless Crawling** : We've implemented a temporary solution to overcome issues with crawling websites in headless mode.\n\nThese changes aim to improve the overall user experience, provide more\nflexibility, and enhance the project's performance. We're thrilled to share\nthese updates with you and look forward to continuing to evolve and improve\nour project!\n\n## [0.2.71] - 2024-06-26\n\n**Improved Error Handling and Performance** ðŸš§\n\n  * ðŸš« Refactored `crawler_strategy.py` to handle exceptions and provide better error messages, making it more robust and reliable.\n  * ðŸ’» Optimized the `get_content_of_website_optimized` function in `utils.py` for improved performance, reducing potential bottlenecks.\n  * ðŸ’» Updated `utils.py` with the latest changes, ensuring consistency and accuracy.\n  * ðŸš« Migrated to `ChromeDriverManager` to resolve Chrome driver download issues, providing a smoother user experience.\n\nThese changes focus on refining the existing codebase, resulting in a more\nstable, efficient, and user-friendly experience. With these improvements, you\ncan expect fewer errors and better performance in the crawler strategy and\nutility functions.\n\n## [0.2.71] - 2024-06-25\n\n### Fixed\n\n  * Speed up twice the extraction function.\n\n## [0.2.6] - 2024-06-22\n\n### Fixed\n\n  * Fix issue #19: Update Dockerfile to ensure compatibility across multiple platforms.\n\n## [0.2.5] - 2024-06-18\n\n### Added\n\n  * Added five important hooks to the crawler:\n  * on_driver_created: Called when the driver is ready for initializations.\n  * before_get_url: Called right before Selenium fetches the URL.\n  * after_get_url: Called after Selenium fetches the URL.\n  * before_return_html: Called when the data is parsed and ready.\n  * on_user_agent_updated: Called when the user changes the user_agent, causing the driver to reinitialize.\n  * Added an example in `quickstart.py` in the example folder under the docs.\n  * Enhancement issue #24: Replaced inline HTML tags (e.g., DEL, INS, SUB, ABBR) with textual format for better context handling in LLM.\n  * Maintaining the semantic context of inline tags (e.g., abbreviation, DEL, INS) for improved LLM-friendliness.\n  * Updated Dockerfile to ensure compatibility across multiple platforms (Hopefully!).\n\n## [0.2.4] - 2024-06-17\n\n### Fixed\n\n  * Fix issue #22: Use MD5 hash for caching HTML files to handle long URLs\n\n"
    },
    {
      "title": "Contact",
      "url": "https://crawl4ai.com/mkdocs/contact/",
      "content": "# Contact\n\nIf you have any questions, suggestions, or feedback, please feel free to reach\nout to us:\n\n  * GitHub: [unclecode](https://github.com/unclecode)\n  * Twitter: [@unclecode](https://twitter.com/unclecode)\n  * Website: [crawl4ai.com](https://crawl4ai.com)\n\n## Contributing ðŸ¤\n\nWe welcome contributions from the open-source community to help improve\nCrawl4AI and make it even more valuable for AI enthusiasts and developers. To\ncontribute, please follow these steps:\n\n  1. Fork the repository.\n  2. Create a new branch for your feature or bug fix.\n  3. Make your changes and commit them with descriptive messages.\n  4. Push your changes to your forked repository.\n  5. Submit a pull request to the main repository.\n\nFor more information on contributing, please see our [contribution\nguidelines](https://github.com/unclecode/crawl4ai/blob/main/CONTRIBUTING.md).\n\n## License ðŸ“„\n\nCrawl4AI is released under the [Apache 2.0\nLicense](https://github.com/unclecode/crawl4ai/blob/main/LICENSE).\n\nLet's work together to make the web more accessible and useful for AI\napplications! ðŸ’ªðŸŒðŸ¤–\n\n"
    }
  ]
}